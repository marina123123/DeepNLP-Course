{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 14 - Pretrained Models.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Z4WlMyJVRkzQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 -qq install torch==0.4.1\n",
        "!pip -qq install torchtext==0.3.1\n",
        "!pip -qq install spacy==2.0.16\n",
        "!pip -qq install gensim==3.6.0\n",
        "!pip -qq install allennlp==0.7.2\n",
        "!pip -qq install pytorch-pretrained-bert==0.1.2\n",
        "!python -m spacy download en\n",
        "\n",
        "!git clone https://github.com/rowanz/swagaf.git\n",
        "!wget -O conll_2003.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1z7swIvfWs97lKkTpHKIeV7Cgv4NKxGC0\"\n",
        "!unzip conll_2003.zip\n",
        "!wget -qq https://raw.githubusercontent.com/yandexdataschool/nlp_course/master/week08_multitask/conlleval.py\n",
        "!wget -O fintech.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=110Mi9nF0J_FTv1MHhf1G-FsZIWEErMzK\"\n",
        "!unzip fintech.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UvJKy3mtVOpw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "    DEVICE = torch.device('cuda')\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "    DEVICE = torch.device('cpu')\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jQXfZ9MCmZBb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pretrained Models"
      ]
    },
    {
      "metadata": {
        "id": "h5YtUOd_m1eR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Один из самых заметных трендов в NLP в 2018 году - использование предобученных языковых моделей как составных частей в моделях, тренируемых под конкретные задачи.\n",
        "\n",
        "Вообще-то мы уже хорошо знакомы с таким подходом: word2vec - это тоже языковая модель, но очень простая. Разница в том, что от эмбеддингов слов без контекста мы переходим к эмбеддингам слов в контексте.\n",
        "\n",
        "Достаточно пафосное объяснение, почему это важно: [NLP's ImageNet moment has arrived](http://ruder.io/nlp-imagenet/).\n",
        "\n",
        "При этом возрастает качество - и падает скорость."
      ]
    },
    {
      "metadata": {
        "id": "KST58kDCovBH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Universal Sentence Encoder"
      ]
    },
    {
      "metadata": {
        "id": "MJpNBSkkq5vm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Начнем немного не с языковой модели: [Universal Sentence Encoder](https://arxiv.org/pdf/1803.11175.pdf). Данная модель предобучалась в режиме multi-task learning: энкодер учился выдавать \"универсальные\" представления предложений, по которым специфичные для каждой задачи декодеры учились  таким вещам как предсказание предыдущего и следующего предложения (Skip-Thoughts like модель) или обычной классификации на размеченных данных.\n",
        "\n",
        "Основной понт в том, что представления получаются вполне себе интепретируемые даже без какого-то специального дообучения под нужную задачу.\n",
        "\n",
        "*К сожалению, я не умею в эту модель на pytorch, поэтому придется с tensorflow жить...*"
      ]
    },
    {
      "metadata": {
        "id": "nmVS7Tdconz_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "\n",
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "universal_sentence_encoder = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder/2\", trainable=False)\n",
        "sess.run([tf.global_variables_initializer(), tf.tables_initializer()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C9WZ_IacoueT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "inputs = tf.placeholder(tf.string, shape=[None])\n",
        "outputs = universal_sentence_encoder(inputs)\n",
        "\n",
        "lines = [\n",
        "    \"How old are you?\",                                                                 # 0\n",
        "    \"Attempting to light a cigarette, someone fumbles with the lighter and drops it.\",  # 1\n",
        "    \"This is the story of a man named Neil Fisk, and how he came to love God.\",         # 2\n",
        "    \"What is your age?\",                                                                # 3\n",
        "    \"Do you have a moment to talk about our Lord?\",                                     # 4\n",
        "]\n",
        "\n",
        "result = sess.run(outputs, {\n",
        "    inputs: lines\n",
        "})\n",
        "\n",
        "plt.title('phrase similarity')\n",
        "plt.imshow(result.dot(result.T), interpolation='none', cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dhUsLpQRtlEp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Например, фразы \"How old are you?\" и \"What is your age?\" не имеют общих слов, но их косинусная близость достаточно высока. Аналогично со второй и четвертой фразами.\n",
        "\n",
        "Обратите внимание, что векторы на выходе из модели уже нормированы - поэтому косинусная близость считается как скалярное произведение.\n",
        "\n",
        "Как пример использования этих представлений почти бесплатно, порешаем такую (упоротую) задачу, утащенную у другого курса: https://www.kaggle.com/c/fintech-tinkoff"
      ]
    },
    {
      "metadata": {
        "id": "NHAD5rL_5pTi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_data = pd.read_csv('train.csv')\n",
        "test_data = pd.read_csv('test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LuB3jS_U5zXU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Дано 60 тысяч пар похожих вопросов + куча вопросов из категории \"другое\". Нужно определить, к какой паре ближе данный вопрос или сказать, что он из другого."
      ]
    },
    {
      "metadata": {
        "id": "txbQGR4O5x5A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data.iloc[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sf8xw3856ef0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Модель вполне себе умеет говорить, что вопросы похожи!"
      ]
    },
    {
      "metadata": {
        "id": "EDRd1WMj6aKL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "result = sess.run(outputs, {\n",
        "    inputs: train_data.iloc[:10].text\n",
        "})\n",
        "\n",
        "plt.title('phrase similarity')\n",
        "plt.imshow(result.dot(result.T), interpolation='none', cmap='gray')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Kj8zX_S6wwW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Напишем функцию для подсчета эмбеддингов всех предложений в датасете:"
      ]
    },
    {
      "metadata": {
        "id": "uRBz8AYh6qqX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def calc_vectors(data):\n",
        "    BATCH_SIZE = 1024\n",
        "\n",
        "    vectors = []\n",
        "    for batch_begin in range(0, len(data), BATCH_SIZE):\n",
        "        batch_end = min(len(data), batch_begin + BATCH_SIZE)\n",
        "\n",
        "        vectors.append(\n",
        "            sess.run(outputs, {\n",
        "                inputs: data.iloc[batch_begin: batch_end].text\n",
        "            })\n",
        "        )\n",
        "\n",
        "    return np.concatenate(vectors, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CmGq2dFE67F8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_vectors = calc_vectors(train_data)\n",
        "train_labels = train_data['labels'].values\n",
        "\n",
        "test_vectors = calc_vectors(test_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ML59PBun6_iN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for test_ind in range(10):\n",
        "    print(test_data.iloc[test_ind].text, train_data.iloc[(train_vectors * test_vectors[test_ind]).sum(-1).argmax()].text, sep='\\t')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HpfewmpJ7ogN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** У нас есть база из векторов и соответствующих им меток. Реализуйте 1NN - поиск ближайшего соседа для запросов из тестовой выборки.\n",
        "\n",
        "В качестве метки тогда можно взять метку этого ближайшего соседа."
      ]
    },
    {
      "metadata": {
        "id": "K_kV7t5H8JKf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yHxji0I88J1F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Вообще, чтобы делать поиск быстро по большой базе применяют приближенные алгоритмы типа: [HNSW](https://github.com/nmslib/hnswlib). В данном случае это не актуально, но всё равно можно попробовать заменить свой 1NN на 2NN из той библиотеки."
      ]
    },
    {
      "metadata": {
        "id": "6OUA2XWet3mj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## ELMo"
      ]
    },
    {
      "metadata": {
        "id": "9J9kpe8Lt-94",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Другая история в случае с [ELMo](https://arxiv.org/pdf/1802.05365.pdf). Это обычная языковая модель:\n",
        "\n",
        "![](https://i.ibb.co/dpp00wG/elmo.png)  \n",
        "*From [Improving a Sentiment Analyzer using ELMo — Word Embeddings on Steroids](http://www.realworldnlpbook.com/blog/improving-sentiment-analyzer-using-elmo.html)*\n",
        "\n",
        "Ну, почти обычная. Во-первых, эмбеддинги слов строятся с помощью сверточной сети над символами.\n",
        "\n",
        "Во-вторых, учится сразу две языковых модели, forward и backward, которые потом конкатенируются.\n",
        "\n",
        "Наконец, в-третьих, смысл модели, что она выдает эмбеддинг слова с учетом его контекста. Это мог бы быть выход последнего слоя LSTM, но ребята поступили хитрее: для каждого слова у нас есть сразу несколько эмбеддингов: выходы каждого из слоев LSTM + выход сверточной сети над символами. При обучении итоговой модели под нужную задачу эмбеддинг слова считается как взвешенная сумма данных эмбеддингов. Веса учатся под задачу.\n",
        "\n",
        "В итоге, когда мы хотим использовать ELMo в своей модели, мы берем эту вот языковую модель и подставляем вместо наших обычных эмбеддингов. Всё, пара строчек изменений - но более хорошие и сильно более медленные эмбеддинги.\n",
        "\n",
        "Языковую модель можно доучивать под задачу - тогда придется дообучать все эти миллионы параметров, что медленно. А можно не доучивать - тогда внутри ELMo будет учится только `(num_layers + 1)` параметр - веса смеси эмбеддингов."
      ]
    },
    {
      "metadata": {
        "id": "OnDv4lNTwlWz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### NER\n",
        "\n",
        "Такие эмбеддинги можно использовать где угодно, но лучше всего они смотрятся в задачах, связанных с разметкой последовательностей (там критичнее всего получать эмбеддинги с учетом контекста; тогда как Universal Sentence Encoder в задачах классификации смотрится логичнее ELMo)."
      ]
    },
    {
      "metadata": {
        "id": "aXsVTn7frq2V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def read_dataset(path):\n",
        "    data = []\n",
        "    with open(path) as f:\n",
        "        words, tags = [], []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line and words:\n",
        "                data.append((words, tags))\n",
        "                words, tags = [], []\n",
        "                continue\n",
        "            word, pos_tag, synt_tag, ner_tag = line.split()\n",
        "            words.append(word)\n",
        "            tags.append(ner_tag)\n",
        "        if words:\n",
        "            data.append((words, tags))\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qLPzgKJ7s8cd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data = read_dataset('train.txt')\n",
        "val_data = read_dataset('valid.txt')\n",
        "test_data = read_dataset('test.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mJ5_M6Nhxxwm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Мы уже смотрели на NER, но вообще он такой:"
      ]
    },
    {
      "metadata": {
        "id": "XKAkEGvjtEZB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data[:3]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UoVVubMIyBCq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Соберем датасет:"
      ]
    },
    {
      "metadata": {
        "id": "aW8WYM3Zrqeh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchtext.data import Field, Example, Dataset, BucketIterator\n",
        "\n",
        "tokens_field = Field(unk_token=None, batch_first=True)\n",
        "tags_field = Field(unk_token=None, batch_first=True)\n",
        "\n",
        "fields = [('tokens', tokens_field), ('tags', tags_field)]\n",
        "\n",
        "train_dataset = Dataset([Example.fromlist(example, fields) for example in train_data], fields)\n",
        "val_dataset = Dataset([Example.fromlist(example, fields) for example in val_data], fields)\n",
        "test_dataset = Dataset([Example.fromlist(example, fields) for example in test_data], fields)\n",
        "\n",
        "tokens_field.build_vocab(train_dataset, val_dataset, test_dataset)\n",
        "tags_field.build_vocab(train_dataset)\n",
        "\n",
        "print('Vocab size =', len(tokens_field.vocab))\n",
        "print('Tags count =', len(tags_field.vocab))\n",
        "\n",
        "train_iter, val_iter, test_iter = BucketIterator.splits(\n",
        "    datasets=(train_dataset, val_dataset, test_dataset), batch_sizes=(32, 128, 128), \n",
        "    shuffle=True, device=DEVICE, sort=False\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KJEwsOXqvjUB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*Обратите внимание: словарь строится по всем датасетам. Не то, чтобы это очень этично, но мы не будем доучивать никакие эмбеддинги - поэтому ничего нечестного.*"
      ]
    },
    {
      "metadata": {
        "id": "_D02bmmzyWOs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Baseline\n",
        "\n",
        "Для начала обучим модель с предобученными словными эмбеддингами:"
      ]
    },
    {
      "metadata": {
        "id": "NBrt0DZkbq3c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_-MvdZmecYRC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embeddings = np.zeros((len(tokens_field.vocab), w2v_model.vectors.shape[1]))\n",
        "\n",
        "for i, token in enumerate(tokens_field.vocab.itos):\n",
        "    if token.lower() in w2v_model.vocab:\n",
        "        embeddings[i] = w2v_model.get_vector(token.lower())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RzVHzUqwywF5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Допишите модель простого теггера.  \n",
        "Обратите внимание `batch_first=True` в `fields` (это для ELMo понадобится)."
      ]
    },
    {
      "metadata": {
        "id": "jvQkY09FuY7_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BaselineTagger(nn.Module):\n",
        "    def __init__(self, embeddings, tags_count, emb_dim=100, rnn_dim=256, num_layers=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        <init layers>\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        <apply layers>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OIX-u6i0y1gN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Допишите тренировщик модели."
      ]
    },
    {
      "metadata": {
        "id": "G1ORojhZvRlk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ModelTrainer():\n",
        "    def __init__(self, model, criterion, optimizer):\n",
        "        self._model = model\n",
        "        self._criterion = criterion\n",
        "        self._optimizer = optimizer\n",
        "        \n",
        "    def on_epoch_begin(self, is_train, name, batches_count):\n",
        "        \"\"\"\n",
        "        Initializes metrics\n",
        "        \"\"\"\n",
        "        self._epoch_loss = 0\n",
        "        self._correct_count, self._total_count = 0, 0\n",
        "        self._is_train = is_train\n",
        "        self._name = name\n",
        "        self._batches_count = batches_count\n",
        "        \n",
        "        self._model.train(is_train)\n",
        "        \n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"\n",
        "        Outputs final metrics\n",
        "        \"\"\"\n",
        "        return '{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "            self._name, self._epoch_loss / self._batches_count, self._correct_count / self._total_count\n",
        "        )\n",
        "        \n",
        "    def on_batch(self, batch):\n",
        "        \"\"\"\n",
        "        Performs forward and (if is_train) backward pass with optimization, updates metrics\n",
        "        \"\"\"        \n",
        "        loss = <calc loss>\n",
        "        correct_count, total_count = <and this stuff>\n",
        "        \n",
        "        self._correct_count += correct_count\n",
        "        self._total_count += total_count\n",
        "        self._epoch_loss += loss.item()\n",
        "        \n",
        "        if self._is_train:\n",
        "            self._optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(self._model.parameters(), 1.)\n",
        "            self._optimizer.step()\n",
        "\n",
        "        return '{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "            self._name, loss.item(), correct_count / total_count\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_VlSd9DCwKoR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Воспользуемся уже знакомой функцией для оценки теггера:"
      ]
    },
    {
      "metadata": {
        "id": "A46PovRQwwaF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from conlleval import evaluate\n",
        "\n",
        "def eval_tagger(model, test_iter):\n",
        "    true_seqs, pred_seqs = [], []\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in test_iter:\n",
        "            logits = model(batch.tokens)\n",
        "            preds = logits.argmax(-1)\n",
        "\n",
        "            seq_lengths = (batch.tags != 0).sum(-1)\n",
        "\n",
        "            for i, seq_len in enumerate(seq_lengths):\n",
        "                true_seqs.append(' '.join(tags_field.vocab.itos[ind] for ind in batch.tags[i, :seq_len]))\n",
        "                pred_seqs.append(' '.join(tags_field.vocab.itos[ind] for ind in preds[i, :seq_len]))\n",
        "\n",
        "    print('Precision = {:.2f}%, Recall = {:.2f}%, F1 = {:.2f}%'.format(*evaluate(true_seqs, pred_seqs, verbose=False)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2B5gBXVTwNIw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "tqdm.get_lock().locks = []\n",
        "\n",
        "\n",
        "def do_epoch(trainer, data_iter, is_train, name=None):\n",
        "    trainer.on_epoch_begin(is_train, name, batches_count=len(data_iter))\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=len(data_iter)) as progress_bar:\n",
        "            for i, batch in enumerate(data_iter):\n",
        "                batch_progress = trainer.on_batch(batch)\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description(batch_progress)\n",
        "                \n",
        "            epoch_progress = trainer.on_epoch_end()\n",
        "            progress_bar.set_description(epoch_progress)\n",
        "            progress_bar.refresh()\n",
        "\n",
        "            \n",
        "def fit(trainer, train_iter, epochs_count=1, val_iter=None):\n",
        "    best_val_loss = None\n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        do_epoch(trainer, train_iter, is_train=True, name=name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_iter is None:\n",
        "            do_epoch(trainer, val_iter, is_train=False, name=name_prefix + '  Val:')\n",
        "            eval_tagger(trainer._model, val_iter)\n",
        "            print(flush=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KQqkOaDJ0oNm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Запустим обучение модели:"
      ]
    },
    {
      "metadata": {
        "id": "TW-efkm_wNAr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = BasicTagger(len(tokens_field.vocab), tags_count=len(tags_field.vocab)).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "trainer = ModelTrainer(model, criterion, optimizer)\n",
        "\n",
        "fit(trainer, train_iter, epochs_count=32, val_iter=val_iter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WiaypapRzENE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### ELMo Model"
      ]
    },
    {
      "metadata": {
        "id": "_TaJcmSYzOmZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Возьмем предобученную модель от авторов (описание работы с ней есть в [ELMo how to](https://github.com/allenai/allennlp/blob/master/tutorials/how_to/elmo.md))."
      ]
    },
    {
      "metadata": {
        "id": "KDy9eHUB2a76",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
        "\n",
        "options_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
        "weight_file = \"https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
        "\n",
        "elmo = Elmo(options_file, weight_file, num_output_representations=1,\n",
        "            dropout=0, vocab_to_cache=tokens_field.vocab.itos).to(DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P6veC-uyzsZY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Вообще надо сначала преобразовать предложение в символьное представление:"
      ]
    },
    {
      "metadata": {
        "id": "dr1W_aDDzglP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "sentences = [['First', 'sentence', '.'], ['Another', '.']]\n",
        "character_ids = batch_to_ids(sentences).to(DEVICE)\n",
        "\n",
        "character_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "byLMOMtpz6ax",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "А потом засунуть это в модель elmo:"
      ]
    },
    {
      "metadata": {
        "id": "ZwB4k0G1z1QD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "elmo(character_ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lnrdSEWN0uG7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Нас интересует `elmo_representations`.\n",
        "\n",
        "Такой интерфейс не слишком удобный - нужно передавать в `batch_to_ids` строки, а это значит - писать с нуля генератор батчей. Кроме этого, плохо передавать большие батчи на gpu - а с символьным представлением из батча `(batch_size, seq_len)` мы получаем батча в `max_word_len` раз больший. Наконец, рассчет словных эмбеддингов по символам не бесплатный (относительно запроса к таблице эмбеддингов).\n",
        "\n",
        "Поэтому при создании модели мы закэшировали все эмбеддинги: параметр `vocab_to_cache=tokens_field.vocab.itos`.\n",
        "\n",
        "Чтобы пользоваться закэшированными значениями в `inputs` будем передавать какой-то мусор, а в `word_inputs` - индексы эмбеддингов."
      ]
    },
    {
      "metadata": {
        "id": "NiaqSTFz_e8W",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_iter))\n",
        "\n",
        "elmo(inputs=batch.tokens.new_empty((batch.tokens.shape[0], batch.tokens.shape[1], 50)), \n",
        "     word_inputs=batch.tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HMk_T5gy_t0b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Обновите модель, заменив эмбеддинги на ELMo."
      ]
    },
    {
      "metadata": {
        "id": "zSU3PjpewM1V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ELMoTagger(nn.Module):\n",
        "    def __init__(self, elmo, tags_count, emb_dim=1024, rnn_dim=256, num_layers=1):\n",
        "        super().__init__()\n",
        "        \n",
        "        <init layers>\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        <apply layers>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xFUGuPqDAu4h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = ELMoTagger(elmo, tags_count=len(tags_field.vocab)).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "trainer = ModelTrainer(model, criterion, optimizer)\n",
        "\n",
        "fit(trainer, train_iter, epochs_count=32, val_iter=val_iter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OruZBPUq_0dR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### CRF\n",
        "\n",
        "Это не имеет отношение к происходящему, вообще говоря, но тоже полезная информация: чтобы лучше работать с задачей предсказания тегов последовательности можно вместо независимого предсказания каждого тега в отдельности предсказывать тег при условии предыдущего тега.\n",
        "\n",
        "Это выглядит как декодер в Seq2Seq моделях - только вместо предыдущего слова на вход подается предыдущий тег.\n",
        "\n",
        "Такое предсказание можно реализовать разными способами. Один из - использование CRF (Conditional Random Field) (Хорошее описание есть [здесь](http://www.davidsbatista.net/blog/2017/11/13/Conditional_Random_Fields/).\n",
        "\n",
        "У нас сейчас есть модель с полносвязным выходным слоем. На каждом шаге по его выходу оценивается вероятность того, какой тег имеет данное слово - просто нормализуются скоры с помощью softmax: $p[i]= \\frac{e^{s[i]}}{\\sum_{j=1}^9 e^{s[j]}}$.\n",
        "\n",
        "А давайте вместо локальной нормализации, делать глобальную, на всю последовательность. Кроме этого, давайте учить вероятности перехода из одного тега на предыдущем шаге в другой тег на следующем $T[y_t, y_{t+1}]$. Например, должно выучиться, что вероятность после `O` встретиться `I-LOC` нулевая (`I-LOC` может быть только после `B-LOC` или другого `I-LOC`).\n",
        "\n",
        "Тогда каждая последовательность будет оцениваться по такой формуле:\n",
        "$$\\begin{align*}\n",
        "C(y_1, \\ldots, y_m) &= b[y_1] &+ \\sum_{t=1}^{m} s_t [y_t] &+ \\sum_{t=1}^{m-1} T[y_{t}, y_{t+1}] &+ e[y_m]\\\\\n",
        "                    &= \\text{begin} &+ \\text{scores} &+ \\text{transitions} &+ \\text{end}\n",
        "\\end{align*}$$\n",
        "\n",
        "Например, могут быть два варианта последовательности:\n",
        "\n",
        "![](https://guillaumegenthial.github.io/assets/crf1.png =x200) ![](https://guillaumegenthial.github.io/assets/crf2.png =x200)   \n",
        "*From [Sequence Tagging with Tensorflow](https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html)*\n",
        "\n",
        "Лучше из них та, у которой ниже сумма скоров тегов + сумма переходов между тегами.\n",
        "\n",
        "В данном случае нужно просто добавить модуль ConditionalRandomField - он будет выучивать переходы $T$, считать лосс глобальный на всю последовательность, а также делать декодинг.\n",
        "\n",
        "Подсчет лосса осуществляется таким образом:\n",
        "```python\n",
        "crf = ConditionalRandomField(tags_count)\n",
        "loss = -crf(output, tags, mask)\n",
        "```\n",
        "где `output` - выход полносвязного слоя, который раньше был последним.\n",
        "\n",
        "Декодинг делается так:\n",
        "``` python\n",
        "decoded_sequences = crf.viterbi_tags(output, mask)\n",
        "```\n",
        "\n",
        "**Задание** Обновите теггер и функции обучения с оценкой качества теггера."
      ]
    },
    {
      "metadata": {
        "id": "AlnBFTUWc4Gt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from allennlp.modules import ConditionalRandomField\n",
        "\n",
        "class CRFTagger(nn.Module):\n",
        "    def __init__(self, embeddings, tags_count, emb_dim=100, rnn_dim=256, num_layers=1):\n",
        "        super().__init__()\n",
        "        <init layers (embeddings can be from either glove or elmo)>\n",
        "        \n",
        "    def forward(self, inputs, mask, tags=None):\n",
        "        <apply layers like in previos models>\n",
        "        \n",
        "        if tags is not None:\n",
        "            return <crf loss>\n",
        "        return <viterbi decoding>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qKFL_N7aF_eE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## BERT\n",
        "\n",
        "Наконец, третий вариант модели (что-то среднее с кучей своих плюшек) - это BERT (да, чуваки умеют называть свои модели).\n",
        "\n",
        "Сходство с ELMo - это тоже языковая модель. Отличия:\n",
        "1. Это Transformer, а не BiLSTM.\n",
        "![](https://i.ibb.co/PQ4qXtr/2018-12-22-21-13-14.png)\n",
        "Чтобы учить языковую модель в рамках трансформера они случайно выкидывали некоторые слова из предложения и пытались предсказывать их с помощью сети. Таким образом, при предсказании токена был доступен весь контекст, а не только левый или только правый, как в ELMo.\n",
        "\n",
        "2. Использовалась дополнительная задача в стиле Skip-Thoughts - предсказание следующего предложения:\n",
        "![](https://i.ibb.co/WWdwmPD/2018-12-22-21-12-59.png =x250)\n",
        "\n",
        "Пара предложений записывалась подряд (внимание на SEP) и модель училась предсказывать, идут ли они подряд в реальности (внимание на CLS).\n",
        "\n",
        "В итоге эмбеддинг CLS выучивался таким, что в нем содержалась информация обо всем контексте - а дальше его легко можно использовать для классификации, как в первой модели. Но при этом выучиваются еще и контекстные эмбеддинги всех слов в предложении, поэтому модель можно использовать аналогично ELMo в задаче теггирования.\n",
        "\n",
        "BERT замечателен тем, что побил результаты всех существующих на данный момент моделей, а также human performance на некоторых задачах ([например, SQuAD 1](https://rajpurkar.github.io/SQuAD-explorer/)).\n",
        "\n",
        "Воспользуемся им для задачи [Swag](https://rowanzellers.com/swag/) - выбор правильного продолжения для текста. Датасет, как следует из названия, формировали таким образом, чтобы это было сложно сделать для бездушной машины (но через несколько месяцев вышел BERT и всё насмарку :( )."
      ]
    },
    {
      "metadata": {
        "id": "tuvwIAN6Ktfx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_data = pd.read_csv('swagaf/data/train.csv')\n",
        "val_data = pd.read_csv('swagaf/data/val.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3ueTJGlcKuA3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data.sample(10)[['startphrase', 'sent1', 'sent2', 'ending0', 'ending1', 'ending2', 'ending3', 'label']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "siUT4-SOKyX-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Обратите внимание на токенизатор - модель работает с подсловами как в bpe."
      ]
    },
    {
      "metadata": {
        "id": "_CmRl9Moz2j7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pytorch_pretrained_bert import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "print(tokenizer.vocab)\n",
        "\n",
        "tokens = tokenizer.tokenize('Why do I do this stuff...')\n",
        "print(tokens)\n",
        "print(tokenizer.convert_tokens_to_ids(tokens))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ameGUADW0N3j",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Соберем датасет."
      ]
    },
    {
      "metadata": {
        "id": "KbDtPRD6KxR-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def collect_samples(data, tokenizer):\n",
        "    choices_list, labels_list, segment_ids_list = [], [], []\n",
        "    \n",
        "    for _, row in tqdm(data.iterrows(), total=len(data)):\n",
        "        first_phrase = tokenizer.tokenize(row.sent1)\n",
        "        second_phrase = tokenizer.tokenize(row.sent2)\n",
        "        choices, segment_ids = [], []\n",
        "        for i, ending in enumerate(row[['ending0', 'ending1', 'ending2', 'ending3']]):\n",
        "            ending = tokenizer.tokenize(ending)\n",
        "            tokens = [\"[CLS]\"] + first_phrase + [\"[SEP]\"] + second_phrase + ending + [\"[SEP]\"]\n",
        "            tokens = tokenizer.convert_tokens_to_ids(tokens)\n",
        "            choices.append(tokens)\n",
        "            \n",
        "            segment_ids.append([0] * (len(first_phrase) + 2) + [1] * (len(tokens) - len(first_phrase) - 2))\n",
        "            \n",
        "        choices_list.append(choices)\n",
        "        segment_ids_list.append(segment_ids)\n",
        "        labels_list.append(row.label)\n",
        "\n",
        "    return np.array(choices_list), np.array(labels_list), np.array(segment_ids_list)\n",
        "\n",
        "\n",
        "train_data, train_labels, train_segment_ids = collect_samples(train_data, tokenizer)\n",
        "val_data, val_labels, val_segment_ids = collect_samples(val_data, tokenizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c8Bd8Z8Wze3k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Будем сэмплы формировать также, как было при обучении модели - `[CLS]<first text>[SEP]<next possible text>[SEP]`.\n",
        "\n",
        "Чтобы модель знала, где заканчивается первый сегмент и начинается второй передается заодно еще `train_segment_ids` - 0 соответствует токену из первого сегмента, а 1 - из второго."
      ]
    },
    {
      "metadata": {
        "id": "eJbfkFB4zYeD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_data[:2], train_labels[:2], train_segment_ids[:2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mhwbwINiK2Fh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Итератор батчей таки придется написать свой..."
      ]
    },
    {
      "metadata": {
        "id": "sArV5QNOLOOx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "\n",
        "def to_matrix(choices_list):\n",
        "    batch_size = len(choices_list)\n",
        "    num_options = len(choices_list[0])\n",
        "    seq_len = max(len(choice) for choices in choices_list for choice in choices)\n",
        "    \n",
        "    matrix = np.zeros((batch_size, num_options, seq_len))\n",
        "    for i, choices in enumerate(choices_list):\n",
        "        for j, choice in enumerate(choices):\n",
        "            matrix[i, j, :len(choice)] = choice\n",
        "\n",
        "    return matrix\n",
        "    \n",
        "\n",
        "class BatchIterator():\n",
        "    def __init__(self, data, labels, segment_ids, batch_size, shuffle=True):\n",
        "        self._data = data\n",
        "        self._labels = labels\n",
        "        self._segment_ids = segment_ids\n",
        "        self._num_samples = len(data)\n",
        "        self._batch_size = batch_size\n",
        "        self._shuffle = shuffle\n",
        "        self._batches_count = int(math.ceil(len(data) / batch_size))\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self._batches_count\n",
        "    \n",
        "    def __iter__(self):\n",
        "        return self._iterate_batches()\n",
        "\n",
        "    def _iterate_batches(self):\n",
        "        indices = np.arange(self._num_samples)\n",
        "        if self._shuffle:\n",
        "            np.random.shuffle(indices)\n",
        "\n",
        "        for start in range(0, self._num_samples, self._batch_size):\n",
        "            end = min(start + self._batch_size, self._num_samples)\n",
        "\n",
        "            batch_indices = indices[start: end]\n",
        "            \n",
        "            choices = to_matrix(self._data[batch_indices])\n",
        "            mask = (choices != 0).astype(np.int)\n",
        "            yield {\n",
        "                'choices': choices,\n",
        "                'segment_ids': to_matrix(self._segment_ids[batch_indices]),\n",
        "                'mask': mask,\n",
        "                'label': self._labels[batch_indices]\n",
        "            }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ll77HRDVLUea",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Тренироваться на colab можно только с очень маленьким батчем:"
      ]
    },
    {
      "metadata": {
        "id": "Ck5QcejCLUFQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_iter = BatchIterator(train_data, train_labels, train_segment_ids, 8)\n",
        "val_iter = BatchIterator(val_data, val_labels, val_segment_ids, 8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KgD4Br8CLcU6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Загружаем предобученный BERT:"
      ]
    },
    {
      "metadata": {
        "id": "0mohrzKlLahe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pytorch_pretrained_bert import BertModel\n",
        "\n",
        "bert = BertModel.from_pretrained('bert-base-uncased').to(DEVICE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yrIEAZiYLiLm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Строим такую модель:  \n",
        "![](https://i.ibb.co/JFcwW6D/2018-12-22-21-23-20.png =x400)\n",
        "\n",
        "BERT строит четыре представления $C_0, \\ldots, C_3$ для четырех вариантов продолжения предложения. Будем учить параметр $V$, чьё скалярное произведение с $C_i$ должно быть максимальным для релевантного продолжения.\n",
        "\n",
        "Для этого просто будем использовать кросс-энтропийные потери и считать softmax:\n",
        "$P_i = \\frac{e^{V\\cdot C_i}}{\\sum_j e^{V\\cdot C_j}}$.\n",
        "\n",
        "То есть прикол в том, что единственное специфичное для задачи, что мы учим - это вектор $V$!"
      ]
    },
    {
      "metadata": {
        "id": "o9Xh2f5eNIwo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MultipleChoiceModel(nn.Module):\n",
        "    def __init__(self, bert, num_choices):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._bert = bert\n",
        "        self._num_choices = num_choices\n",
        "        self._dropout = nn.Dropout(0.1)\n",
        "        self._classifier = nn.Linear(768, 1)\n",
        "        \n",
        "    def forward(self, choices, segment_ids, mask):\n",
        "        \"\"\"\n",
        "        choices: LongTensor of shape [batch_size, num_choices, seq_len] with token ids\n",
        "        segment_ids: LongTensor of shape [batch_size, num_choices, seq_len] with token types (0 for first segment, 1 for second)\n",
        "        mask: LongTensor of shape [batch_size, num_choices, seq_len] with mask for padding tokens\n",
        "        returns logits - FloatTensor of shape [batch_size, num_choices]\n",
        "        \"\"\"\n",
        "        choices = choices.view(-1, choices.size(-1))\n",
        "        segment_ids = segment_ids.view(-1, segment_ids.size(-1))\n",
        "        mask = mask.view(-1, mask.size(-1))\n",
        "        \n",
        "        _, pooled_output = self._bert(choices, segment_ids, mask, output_all_encoded_layers=False)\n",
        "        \n",
        "        pooled_output = self._dropout(pooled_output)\n",
        "        logits = self._classifier(pooled_output)\n",
        "        return logits.view(-1, self._num_choices)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GFsvZg501W7S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Доделайте обучалку для модели."
      ]
    },
    {
      "metadata": {
        "id": "AgbnYjwlNLj1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ModelTrainer():\n",
        "    def __init__(self, model, criterion, optimizer):\n",
        "        self._model = model\n",
        "        self._criterion = criterion\n",
        "        self._optimizer = optimizer\n",
        "        \n",
        "    def on_epoch_begin(self, is_train, name, batches_count):\n",
        "        \"\"\"\n",
        "        Initializes metrics\n",
        "        \"\"\"\n",
        "        self._epoch_loss = 0\n",
        "        self._correct_count, self._total_count = 0, 0\n",
        "        self._is_train = is_train\n",
        "        self._name = name\n",
        "        self._batches_count = batches_count\n",
        "        \n",
        "        self._model.train(is_train)\n",
        "        \n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"\n",
        "        Outputs final metrics\n",
        "        \"\"\"\n",
        "        return '{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "            self._name, self._epoch_loss / self._batches_count, self._correct_count / self._total_count\n",
        "        )\n",
        "        \n",
        "    def on_batch(self, batch):\n",
        "        \"\"\"\n",
        "        Performs forward and (if is_train) backward pass with optimization, updates metrics\n",
        "        \"\"\"\n",
        "        \n",
        "        loss = <calc loss>\n",
        "        correct_count, total_count = <and this stuff>\n",
        "        \n",
        "        self._correct_count += correct_count\n",
        "        self._total_count += total_count\n",
        "        self._epoch_loss += loss.item()\n",
        "        \n",
        "        if self._is_train:\n",
        "            self._optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            nn.utils.clip_grad_norm_(self._model.parameters(), 1.)\n",
        "            self._optimizer.step()\n",
        "\n",
        "        return '{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "            self._name, loss.item(), correct_count / total_count\n",
        "        )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8XCHZ6-PNOoa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Нужно также немного магии для инициализации оптимизатора:"
      ]
    },
    {
      "metadata": {
        "id": "R47LnhS4NNS_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from pytorch_pretrained_bert.optimization import BertAdam\n",
        "\n",
        "model = MultipleChoiceModel(bert, 4).to(DEVICE)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "params = [(name, param) for name, param in model.named_parameters() if 'pooler' not in name]\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [param for name, param in params if not any(nd in name for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [param for name, param in params if any(nd in name for nd in no_decay)], 'weight_decay': 0.0}\n",
        "]\n",
        "optimizer = BertAdam(optimizer_grouped_parameters, lr=5e-5, warmup=0.1, t_total=len(train_iter) * 2)\n",
        "\n",
        "trainer = ModelTrainer(model, criterion, optimizer)\n",
        "\n",
        "fit(trainer, train_iter, 2, val_iter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JLmy77H-NdT4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** В память не влезает больший батч. А хочется тренировать с большим батчем (у гугла был 16).\n",
        "\n",
        "Чтобы решить это, есть такой простой (если на pytorch, а не на tensorflow) прием - накапливание градиентов. Можно просто оптимизировать модель не на каждом шаге, а раз в несколько шагов обучения:\n",
        "[Training Neural Nets on Larger Batches: Practical Tips for 1-GPU, Multi-GPU & Distributed setups](https://medium.com/huggingface/training-larger-batches-practical-tips-on-1-gpu-multi-gpu-distributed-setups-ec88c3e51255)\n",
        "\n",
        "Реализуйте такой подход."
      ]
    },
    {
      "metadata": {
        "id": "n2x9-j4oz08p",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Дополнительные материалы\n",
        "\n",
        "## Статьи\n",
        "Universal Language Model Fine-tuning for Text Classification [[pdf]](https://arxiv.org/pdf/1801.06146)  \n",
        "Deep contextualized word representations [[pdf]](https://arxiv.org/pdf/1802.05365)  \n",
        "Improving Language Understanding by Generative Pre-Training [[pdf]](https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf)  \n",
        "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding [[pdf]](https://arxiv.org/pdf/1810.04805.pdf)\n",
        "\n",
        "Dissecting Contextual Word Embeddings: Architecture and Representation [[pdf]](http://aclweb.org/anthology/D18-1179)\n",
        "\n",
        "## Блоги\n",
        "[NLP's ImageNet moment has arrived](http://ruder.io/nlp-imagenet/)  \n",
        "[The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)](https://jalammar.github.io/illustrated-bert/)  \n",
        "[Improving a Sentiment Analyzer using ELMo — Word Embeddings on Steroids](http://www.realworldnlpbook.com/blog/improving-sentiment-analyzer-using-elmo.html)\n",
        "\n",
        "[Sequence Tagging with Tensorflow](https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html)  \n",
        "[Conditional Random Field Tutorial in PyTorch](https://towardsdatascience.com/conditional-random-field-tutorial-in-pytorch-ca0d04499463)  \n",
        "[Conditional Random Fields for Sequence Prediction](http://www.davidsbatista.net/blog/2017/11/13/Conditional_Random_Fields/)"
      ]
    },
    {
      "metadata": {
        "id": "gjkS1Kpkz4Aa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Сдача\n",
        "\n",
        "[Форма для сдачи](https://goo.gl/forms/UvDRQh1CAR3R90No2)  \n",
        "[Feedback](https://goo.gl/forms/9aizSzOUrx7EvGlG3)"
      ]
    }
  ]
}