{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 03 - Word Embeddings (Part 2).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "9vacV4BIFI8l",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 -qq install torch==0.4.1\n",
        "!pip install -q --upgrade nltk gensim bokeh pandas\n",
        "\n",
        "!wget -O quora.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1ERtxpdWOgGQ3HOigqAMHTJjmOE_tWvoF\"\n",
        "!unzip quora.zip\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XIFSTdJG95SZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import clear_output\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim \n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lbpWIAreB6ky",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Введение в PyTorch"
      ]
    },
    {
      "metadata": {
        "id": "_M0mMOadG8aZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "PyTorch - это один из самых известных фреймворков для работы с нейронными сетями.\n",
        "\n",
        "Почему именно он? Ну, он няшен, питоняч и проще в отладке - по сравнению с монстрами типа tensoflow (хотя tf 2.0 с eager execution будет примерно таким же).\n",
        "\n",
        "И вообще, мы тут не фреймворки, а сеточки учить собирались :)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "vsScdJ7DLZCm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Автоматическое дифференцирование"
      ]
    },
    {
      "metadata": {
        "id": "bY9FHLM-M4aW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Графы вычислений"
      ]
    },
    {
      "metadata": {
        "id": "KkvCloDpNXdH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Графы вычислений - это такой удобный способ быстро считать градиенты сложных-пресложных функций.\n",
        "\n",
        "Например, функция\n",
        "\n",
        "$$f = (x + y) \\cdot z$$\n",
        "\n",
        "представится графом\n",
        "\n",
        "![graph](https://github.com/DanAnastasyev/DeepNLP-Course/raw/master/Week%2003/Images/Circuit.png =500x)  \n",
        "*From [Backpropagation, Intuitions - CS231n](http://cs231n.github.io/optimization-2/)*\n",
        "\n",
        "**Задание** Зададим значения $x, y, z$ (зеленым на картинке). Как посчитать $\\frac{\\partial f}{\\partial x}, \\frac{\\partial f}{\\partial y}, \\frac{\\partial f}{\\partial z}$? (*Вспоминаем, что такое backpropagation*)\n",
        "\n",
        "В PyTorch такие вычисления делаются очень просто.\n",
        "\n",
        "Сначала определяется функция - просто последовательность операций:"
      ]
    },
    {
      "metadata": {
        "id": "lw4ASRktLdO4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = torch.tensor(-2., requires_grad=True)\n",
        "y = torch.tensor(5., requires_grad=True)\n",
        "z = torch.tensor(-4., requires_grad=True)\n",
        "\n",
        "q = x + y\n",
        "f = q * z"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-78COM99N8YL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "А затем говорим ей: \"Посчитай градиенты, пожалуйста\". И происходит магия - какая-то такая:\n",
        "\n",
        "![graph](https://raw.githubusercontent.com/pytorch/pytorch/master/docs/source/_static/img/dynamic_graph.gif)  \n",
        "*From [github.com/pytorch/pytorch](https://github.com/pytorch/pytorch)*\n",
        "\n",
        "По описанной последовательности операций *на лету* строится граф вычислений, и обратный проход выполняется по нему.\n",
        "\n",
        "В этом ключевое отличие от tensoflow: граф не нужно компилировать до исполнения кода - это позволяет более гибко управлять его структурой."
      ]
    },
    {
      "metadata": {
        "id": "9FOlPMIQMfbq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f.backward()\n",
        "\n",
        "print('df/dz =', z.grad)\n",
        "print('df/dx =', x.grad)\n",
        "print('df/dy =', y.grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JotDf1naGU-R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Вызов метода `backward()` вычисляет градиенты для всех тензоров, у которых `requires_grad == True`.\n",
        "\n",
        "Есть еще альтернативный способ не вычислять градиенты - пользоваться менеджерами контекста ([Locally disabling gradient computation](https://pytorch.org/docs/stable/autograd.html#locally-disabling-gradient-computation)):\n",
        "```python\n",
        "torch.autograd.no_grad()\n",
        "torch.autograd.enable_grad()\n",
        "torch.autograd.set_grad_enabled(mode)\n",
        "\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "WQEJeqfnJPpA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with torch.autograd.no_grad():\n",
        "    x = torch.tensor(-2., requires_grad=True)\n",
        "    y = torch.tensor(5., requires_grad=True)\n",
        "    q = x + y\n",
        "\n",
        "z = torch.tensor(-4., requires_grad=True)\n",
        "f = q * z\n",
        "\n",
        "f.backward()\n",
        "\n",
        "print('df/dz =', z.grad)\n",
        "print('df/dx =', x.grad)\n",
        "print('df/dy =', y.grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tSiB1CGyJMzt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Подробнее о том, как работает autograd, можно почитать здесь: [Autograd mechanics](https://pytorch.org/docs/stable/notes/autograd.html).\n",
        "\n",
        "В целом, любой тензор в pytorch - аналог многомерных матриц в numpy.\n",
        "\n",
        "Он содержит данные:"
      ]
    },
    {
      "metadata": {
        "id": "DY2CcCw2Gmgq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x.data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zYxD8N_9GpJl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Накопленный градиент:"
      ]
    },
    {
      "metadata": {
        "id": "wYCD5P24GufX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x.grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jwLx4szvGwMb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Функцию, как градиент считать:"
      ]
    },
    {
      "metadata": {
        "id": "TTfGdUF_GzV8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "q.grad_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VgK1Esa6HHAB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "И всякую дополнительную метаинформацию:"
      ]
    },
    {
      "metadata": {
        "id": "Nazaer0AG4pL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x.type(), x.shape, x.device, x.layout"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WvLFlc4iQOQv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Зачем... У меня один вопрос - зачем вот это вот нам нужно?"
      ]
    },
    {
      "metadata": {
        "id": "FlhLBWwHG3Xe",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Задача для разминки"
      ]
    },
    {
      "metadata": {
        "id": "kaqtIIvJOEut",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Чтобы разобраться - решим простенькую задачу на линейную регрессию:"
      ]
    },
    {
      "metadata": {
        "id": "QDZpEHF8AKH2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "w_orig, b_orig = 2.6, -0.4\n",
        "\n",
        "X = np.random.rand(100) * 10. - 5.\n",
        "y_orig = w_orig * X + b_orig\n",
        "\n",
        "y = y_orig + np.random.randn(100)\n",
        "\n",
        "plt.plot(X, y, '.')\n",
        "plt.plot(X, y_orig)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r2K5MVtiSGuC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Хочется прикрутить сюда backpropagation, да.\n",
        "\n",
        "Есть два параметра $w$ и $b$ - их нужно подобрать такими, чтобы они были как можно ближе к исходным $w_{orig}, b_{orig}$.\n",
        "\n",
        "Что будем оптимизировать? Оптимизировать будем MSE:\n",
        "$$J(w, b) = \\frac{1}{N} \\sum_{i=1}^N || \\hat y_i - y_i(w, b)||^2 =\\frac{1}{N} \\sum_{i=1}^N || \\hat y_i - (w \\cdot x_i + b)||^2. $$\n",
        "\n",
        "С такой функций потерь можем запустить простой градиентный спуск (даже не стохастический пока):\n",
        "$$w_{t+1} := w_t - \\alpha \\cdot \\frac{\\partial J}{\\partial w}(w_t, b_t)$$\n",
        "$$b_{t+1} := w_t - \\alpha \\cdot \\frac{\\partial J}{\\partial b}(w_t, b_t)$$\n",
        "\n",
        "**Задание** Реализовать оптимизацию на чистом numpy.\n",
        "\n",
        "Для этого нужно:\n",
        "1. Посчитать значение функции на прямом проходе: $y(w, b) = w \\cdot x + b$;\n",
        "2. Подумать и посчитать градиенты $\\frac{\\partial J}{\\partial w}, \\frac{\\partial J}{\\partial b}$ на обратном проходе;\n",
        "3. Сдвинуть $w, b$ по антиградиентам."
      ]
    },
    {
      "metadata": {
        "id": "VKbqTNVXFB3A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def display_progress(epoch, loss, w, b, X, y, y_pred):\n",
        "    clear_output(True)\n",
        "    print('Epoch = {}, Loss = {}, w = {}, b = {}'.format(epoch, loss, w, b))\n",
        "    plt.plot(X, y, '.')\n",
        "    plt.plot(X, y_pred)\n",
        "    plt.show()\n",
        "    time.sleep(1)\n",
        "\n",
        "\n",
        "w = np.random.randn()\n",
        "b = np.random.randn()\n",
        "\n",
        "alpha = 0.01\n",
        "\n",
        "for i in range(100):\n",
        "    y_pred = <calc it>\n",
        "\n",
        "    loss = <and it>\n",
        "\n",
        "    <find w_grad and b_grad>\n",
        "\n",
        "    w -= alpha * w_grad\n",
        "    b -= alpha * b_grad\n",
        "    \n",
        "    if (i + 1) % 5 == 0:\n",
        "        display_progress(i + 1, loss, w, b, X, y, y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "X8WgWrF4C2WK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "На PyTorch то же самое сделать несколько проще - подсчет прямого прохода копируется почти дословно.\n",
        "\n",
        "Обратный проход мы уже умеем - нужно просто вызвать `loss.backward()`.\n",
        "\n",
        "Для обновления `w` и `b` нужно иметь в виду следующее. Во-первых, pytorch не даст просто так обновить их:"
      ]
    },
    {
      "metadata": {
        "id": "Zx4DoGeBMJd4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "w = torch.randn(1, requires_grad=True)\n",
        "\n",
        "w -= 1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8OjoUh-SMPBt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Проблема в сложности поддержки in-place операций для работы autograd ([In place operations with autograd](https://pytorch.org/docs/stable/notes/autograd.html#in-place-operations-with-autograd)).\n",
        "\n",
        "Но нам и не нужна поддержка градиентов! Мы не будем делать backward pass через эту операцию - нужно всего лишь обновить значение переменной. Чтобы сделать это, можно воспользовать контекстом `no_grad`, либо производить обновление непосредственно буфера, который использует данный тензор:"
      ]
    },
    {
      "metadata": {
        "id": "zegkKd-cMOMj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "w.data -= 1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YVlaIdvHNXR_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Другое, что нужно помнить - градиенты в тензорах накапливаются. Между вызовами `loss.backward()` нужно обнулять градиенты у `w` и `b`:\n",
        "```python\n",
        "w.grad.zero_()\n",
        "b.grad.zero_()\n",
        "```\n",
        "\n",
        "**Задание** Реализовать линейную регрессию на pytorch."
      ]
    },
    {
      "metadata": {
        "id": "VRqxypuEU2ig",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = torch.as_tensor(X).float()\n",
        "y = torch.as_tensor(y).float()\n",
        "\n",
        "w = torch.randn(1, requires_grad=True)\n",
        "b = torch.randn(1, requires_grad=True)\n",
        "\n",
        "for i in range(100):\n",
        "    <copy forward pass and add backward pass + parameters updates>\n",
        "    \n",
        "    if (i + 1) % 5 == 0:\n",
        "        display_progress(i + 1, loss, w.item(), b.item(), \n",
        "                         X.data.numpy(), y.data.numpy(), y_pred.data.numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KaKTKN_fOvo-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Думать нужно уже гораздо меньше, да? :)\n",
        "\n",
        "Про другие фишки низкоуровнего pytorch можно почитать здесь: [PyTorch — ваш новый фреймворк глубокого обучения](https://habr.com/post/334380/) (статья веселая, но немного устарела, читать лучше с оглядкой на [PyTorch 0.4.0 Migration Guide](https://pytorch.org/blog/pytorch-0_4_0-migration-guide/))"
      ]
    },
    {
      "metadata": {
        "id": "pZNq6ujzPtvd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Word embeddings и высокоуровневый API PyTorch"
      ]
    },
    {
      "metadata": {
        "id": "ITLgcVz66AfV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Займёмся рассмотрением высокоуровневого API - в нем уже реализованы разные классы-запчасти для обучения нейронок.\n",
        "\n",
        "Будем решать всё ту же задачу, что и в прошлый раз - обучение словных эмбеддингов, только теперь мы будем учить их самостоятельно!\n",
        "\n",
        "Для начала нужно подготовить данные для обучения.\n",
        "\n",
        "Соберем и токенизируем тексты:"
      ]
    },
    {
      "metadata": {
        "id": "hKKb9Ya8hzIb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "quora_data = pd.read_csv('train.csv')\n",
        "\n",
        "quora_data.question1 = quora_data.question1.replace(np.nan, '', regex=True)\n",
        "quora_data.question2 = quora_data.question2.replace(np.nan, '', regex=True)\n",
        "\n",
        "texts = list(pd.concat([quora_data.question1, quora_data.question2]).unique())\n",
        "\n",
        "tokenized_texts = [word_tokenize(text.lower()) for text in texts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QYoj91iDDDfT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Соберем индекс самых частотных слов:"
      ]
    },
    {
      "metadata": {
        "id": "5PL471pGjuVN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "MIN_COUNT = 5\n",
        "\n",
        "words_counter = Counter(token for tokens in tokenized_texts for token in tokens)\n",
        "word2index = {\n",
        "    '<unk>': 0\n",
        "}\n",
        "\n",
        "for word, count in words_counter.most_common():\n",
        "    if count < MIN_COUNT:\n",
        "        break\n",
        "        \n",
        "    word2index[word] = len(word2index)\n",
        "    \n",
        "index2word = [word for word, _ in sorted(word2index.items(), key=lambda x: x[1])]\n",
        "    \n",
        "print('Vocabulary size:', len(word2index))\n",
        "print('Tokens count:', sum(len(tokens) for tokens in tokenized_texts))\n",
        "print('Unknown tokens appeared:', sum(1 for tokens in tokenized_texts for token in tokens if token not in word2index))\n",
        "print('Most freq words:', index2word[1:21])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DF5mYpCsE9Uh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Skip-Gram Word2vec"
      ]
    },
    {
      "metadata": {
        "id": "om1IG5XEMGRa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Начнем с skip-gram модели обучения word2vec.\n",
        "\n",
        "Это простая модель всего из двух слоев. Ее идея - учить вектора эмбеддингов такими, чтобы по ним можно было как можно лучше предсказать контекст соответствующих слов. То есть если мы хорошо научились кодировать слова, с которыми встречается данное - значит, мы что-то знаем и о нем самом. Например, естественным образом получится, что слова, встречающиеся в одинаковых контекстах (скажем, `apple` и `orange`)  будут иметь близкие вектора эмбеддингов.\n",
        "\n",
        "![](https://github.com/DanAnastasyev/DeepNLP-Course/raw/master/Week%2003/Images/Word2vecExample.jpeg =600x)  \n",
        "*From cs224n, Lecture 2*\n",
        "\n",
        "Для этого мы моделируем вероятности $\\{P(w_{c+j}|w_c):  j = c-k, ..., c+k, j \\neq c\\}$, где $k$ - размер контекстного окна, $c$ - индекс центрального слова.\n",
        "\n",
        "Соберем такую модель: будем учить пару матриц $U$ - матрицу эмбеддингов, которую потом и возьмем для своих задач, и $V$ - матрицу выходного слоя.\n",
        "\n",
        "Каждому слову в словаре соответствует строка в матрице $U$ и столбец $V$.\n",
        "\n",
        "![skip-gram](https://github.com/DanAnastasyev/DeepNLP-Course/raw/master/Week%2003/Images/SkipGram.png =500x)\n",
        "\n",
        "Что тут происходит? Слово отображается в эмбеддинг - строку $u_c$. Дальше этот эмбеддинг умножается на матрицу $V$. \n",
        "\n",
        "В итоге получаем набор числе $v_j^T u_c$ - степень похожести слова с номером $j$ и нашего слова.\n",
        "\n",
        "Преобразуем эти числа в что-то вроде вероятностей - воспользуемся функцией softmax: $P(i) = \\frac{e^{x_i}}{\\sum_j e^{x_j}}$.\n",
        "\n",
        "А дальше будем считать кросс-энтропийные потери:\n",
        "\n",
        "$$-\\sum_{-k \\leq j \\leq k, j \\neq 0} \\log \\frac{\\exp(v_{c+j}^T u_c)}{\\sum_{i=1}^{|V|} \\exp(v_i^T u_c)} \\to \\min_{U, V}.$$\n",
        "\n",
        "В итоге, вектор $u_c$ будет приближаться к векторам $v_{c_j}$ из его контекста.\n",
        "\n",
        "Реализуем это всё, чтобы разобраться.\n",
        "\n",
        "#### Генерация батчей\n",
        "\n",
        "Для начала нужно собрать контексты."
      ]
    },
    {
      "metadata": {
        "id": "ocrsXgaynYPG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def build_contexts(tokenized_texts, window_size)\n",
        "    contexts = []\n",
        "    for tokens in tokenized_texts:\n",
        "        for i in range(len(tokens)):\n",
        "            central_word = tokens[i]\n",
        "            context = [tokens[i + delta] for delta in range(-window_size, window_size + 1) \n",
        "                       if delta != 0 and i + delta >= 0 and i + delta < len(tokens)]\n",
        "\n",
        "            contexts.append((central_word, context))\n",
        "            \n",
        "    return contexts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AQBa6yQ9BXjp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "contexts = build_contexts(tokenized_texts, window_size=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KyQNK-9SBdb9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "contexts[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pbQKln_6yC4l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Преобразуем слова в индексы."
      ]
    },
    {
      "metadata": {
        "id": "hOPRlKlLvUBA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "contexts = [(word2index.get(central_word, 0), [word2index.get(word, 0) for word in context]) \n",
        "            for central_word, context in contexts]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GYmrAi9gyIe-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Реализуем генератор батчей для нашей нейронки:"
      ]
    },
    {
      "metadata": {
        "id": "6opX5cEp8LxC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def make_skip_gram_batchs_iter(contexts, window_size, num_skips, batch_size):\n",
        "    assert batch_size % num_skips == 0\n",
        "    assert num_skips <= 2 * window_size\n",
        "    \n",
        "    central_words = [word for word, context in contexts if len(context) == 2 * window_size and word != 0]\n",
        "    contexts = [context for word, context in contexts if len(context) == 2 * window_size and word != 0]\n",
        "    \n",
        "    batch_size = int(batch_size / num_skips)\n",
        "    batchs_count = int(math.ceil(len(contexts) / batch_size))\n",
        "    \n",
        "    print('Initializing batchs generator with {} batchs per epoch'.format(batchs_count))\n",
        "    \n",
        "    while True:\n",
        "        indices = np.arange(len(contexts))\n",
        "        np.random.shuffle(indices)\n",
        "\n",
        "        for i in range(batchs_count):\n",
        "            batch_begin, batch_end = i * batch_size, min((i + 1) * batch_size, len(contexts))\n",
        "            batch_indices = indices[batch_begin: batch_end]\n",
        "\n",
        "            batch_data, batch_labels = [], []\n",
        "\n",
        "            for data_ind in batch_indices:\n",
        "                central_word, context = central_words[data_ind], contexts[data_ind]\n",
        "                \n",
        "                words_to_use = random.sample(context, num_skips)\n",
        "                batch_data.extend([central_word] * num_skips)\n",
        "                batch_labels.extend(words_to_use)\n",
        "            \n",
        "            yield batch_data, batch_labels"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g0D79MwB_gMe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch, labels = next(make_skip_gram_batchs_iter(contexts, window_size=2, num_skips=2, batch_size=32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6DXjZS3JyQZh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### nn.Sequential\n",
        "\n",
        "Простейший способ реализовать модель на PyTorch - использовать модуль `nn.Sequential`. В нем нужно просто перечислить все слои, и он будет применять их последовательно."
      ]
    },
    {
      "metadata": {
        "id": "WRw9Z4G__46O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Embedding(len(word2index), 32),\n",
        "    nn.Linear(32, len(word2index))\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ysn0DDpLyj1S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Еще одна особенность pytorch, о которой до сих пор не говорили - поддержка вычислений на видеокарте. На видеокарте большинство нейронок считается гораздо быстрее благодаря высокой параллелизации. Сказать pytorch'у, чтобы он считал на видеокарте, очень просто:"
      ]
    },
    {
      "metadata": {
        "id": "EfmaUi3Uy9YT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a3c3UEa2zHhk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "либо"
      ]
    },
    {
      "metadata": {
        "id": "OHxAg5ZWzEKT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\")\n",
        "\n",
        "model = model.to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pHi1CL2pzxOg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Создать тензоры на видеокарте можно, например, так:"
      ]
    },
    {
      "metadata": {
        "id": "Ycx1O3_SzvmC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch = torch.cuda.LongTensor(batch)\n",
        "labels = torch.cuda.LongTensor(labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YtLMvOO2z3c8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Заставить модель посчитать значение можно так:"
      ]
    },
    {
      "metadata": {
        "id": "N9wTpewTz3Dk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "logits = model(batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dWJmDy_uzJgD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Теперь нам нужна функция потерь"
      ]
    },
    {
      "metadata": {
        "id": "h7rlD62_ykYl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_function = nn.CrossEntropyLoss().cuda() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OxLBiBua0OZM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Посчитать значение можно так:"
      ]
    },
    {
      "metadata": {
        "id": "fCaTB5cc0GVw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss = loss_function(logits, labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gAwx-pck0RxX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "А теперь, конечно же, backprop!"
      ]
    },
    {
      "metadata": {
        "id": "JWt6gL0_0Npp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss.backward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZJkDOl6szRLm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "И, наконец, оптимизатор.\n",
        "\n",
        "Будем использовать Adam. Интерфейс - передать список оптимизируемых параметров и learning rate."
      ]
    },
    {
      "metadata": {
        "id": "5-b5CIARzQ6m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.01) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ju5lO0Xi0hsV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Оптимизация идет просто - нужно вызвать `step()`:"
      ]
    },
    {
      "metadata": {
        "id": "_9QK7nHu0Zw8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(model[1].weight)\n",
        "\n",
        "optimizer.step()\n",
        "\n",
        "print(model[1].weight)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Hnxyk1ew0pSk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "И последнее - нужно обнулить градиенты!"
      ]
    },
    {
      "metadata": {
        "id": "uMsuvEP90svi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer.zero_grad()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PibTw33Azg7q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Реализация обучения skip-gram модели\n",
        "\n",
        "Наконец, напишем цикл обучения - как уже было с линейной регрессией.\n",
        "\n",
        " **Задание** Заполните цикл."
      ]
    },
    {
      "metadata": {
        "id": "ewGMgYTXANzz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_every_nsteps = 1000\n",
        "total_loss = 0\n",
        "start_time = time.time()\n",
        "\n",
        "for step, (batch, labels) in enumerate(make_skip_gram_batchs_iter(contexts, window_size=2, num_skips=4, batch_size=128)):\n",
        "    <1. convert data to tensors>\n",
        "    \n",
        "    <2. make forward pass>\n",
        "\n",
        "    <3. make backward pass>\n",
        "\n",
        "    <4. apply optimizer>\n",
        "    \n",
        "    <5. zero grads>\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    \n",
        "    if step != 0 and step % loss_every_nsteps == 0:\n",
        "        print(\"Step = {}, Avg Loss = {:.4f}, Time = {:.2f}s\".format(step, total_loss / loss_every_nsteps, \n",
        "                                                                    time.time() - start_time))\n",
        "        total_loss = 0\n",
        "        start_time = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Pqq9kee41L4P",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Анализ\n",
        "\n",
        "Получить эмбеддинги можно, скаставав такое заклинание:"
      ]
    },
    {
      "metadata": {
        "id": "uWsYkNn-Hnl_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "embeddings = model[0].weight.cpu().data.numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EZtxY2D01RB6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Проверим, получилось ли хоть сколько-то адекватно."
      ]
    },
    {
      "metadata": {
        "id": "bhDwuhDSHEDm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "def most_similar(embeddings, index2word, word2index, word):\n",
        "    word_emb = embeddings[word2index[word]]\n",
        "    \n",
        "    similarities = cosine_similarity([word_emb], embeddings)[0]\n",
        "    top10 = np.argsort(similarities)[-10:]\n",
        "    \n",
        "    return [index2word[index] for index in reversed(top10)]\n",
        "\n",
        "most_similar(embeddings, index2word, word2index, 'warm')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0VS1x-mO1WKS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "И визуализируем!"
      ]
    },
    {
      "metadata": {
        "id": "yuXv2HxsAecb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import bokeh.models as bm, bokeh.plotting as pl\n",
        "from bokeh.io import output_notebook\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.preprocessing import scale\n",
        "\n",
        "\n",
        "def draw_vectors(x, y, radius=10, alpha=0.25, color='blue',\n",
        "                 width=600, height=400, show=True, **kwargs):\n",
        "    \"\"\" draws an interactive plot for data points with auxilirary info on hover \"\"\"\n",
        "    output_notebook()\n",
        "    \n",
        "    if isinstance(color, str): \n",
        "        color = [color] * len(x)\n",
        "    data_source = bm.ColumnDataSource({ 'x' : x, 'y' : y, 'color': color, **kwargs })\n",
        "\n",
        "    fig = pl.figure(active_scroll='wheel_zoom', width=width, height=height)\n",
        "    fig.scatter('x', 'y', size=radius, color='color', alpha=alpha, source=data_source)\n",
        "\n",
        "    fig.add_tools(bm.HoverTool(tooltips=[(key, \"@\" + key) for key in kwargs.keys()]))\n",
        "    if show: \n",
        "        pl.show(fig)\n",
        "    return fig\n",
        "\n",
        "\n",
        "def get_tsne_projection(word_vectors):\n",
        "    tsne = TSNE(n_components=2, verbose=100)\n",
        "    return scale(tsne.fit_transform(word_vectors))\n",
        "    \n",
        "    \n",
        "def visualize_embeddings(embeddings, index2word, word_count):\n",
        "    word_vectors = embeddings[1: word_count + 1]\n",
        "    words = index2word[1: word_count + 1]\n",
        "    \n",
        "    word_tsne = get_tsne_projection(word_vectors)\n",
        "    draw_vectors(word_tsne[:, 0], word_tsne[:, 1], color='green', token=words)\n",
        "    \n",
        "    \n",
        "visualize_embeddings(model[0], index2word, 1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XGfhLR6x8D3r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Continuous Bag of Words (CBoW)"
      ]
    },
    {
      "metadata": {
        "id": "3UuVr2IsaYhX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Альтернативный вариант модели:\n",
        "\n",
        "![](https://github.com/DanAnastasyev/DeepNLP-Course/raw/master/Week%2003/Images/CBOW.png =500x)\n",
        "\n",
        "Теперь по *сумме* контекстных векторов предсказывается вектор центрального слова.\n",
        "\n",
        "**Задание** Реализуйте часть функции для генерации батчей."
      ]
    },
    {
      "metadata": {
        "id": "NP5VmnnjtsXn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_cbow_batchs_iter(contexts, window_size, batch_size):\n",
        "    data = np.array([context for word, context in contexts if len(context) == 2 * window_size and word != 0])\n",
        "    labels = np.array([word for word, context in contexts if len(context) == 2 * window_size and word != 0])\n",
        "        \n",
        "    batchs_count = int(math.ceil(len(data) / batch_size))\n",
        "    \n",
        "    print('Initializing batchs generator with {} batchs per epoch'.format(batchs_count))\n",
        "    \n",
        "    while True:\n",
        "        <do batchs generation>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S9HkY-VO61n3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Посмотрим на альтернативный вариант создания модели - им мы будем пользоваться чаще всего - отнаследоваться от `nn.Module`. Схематично её использование выглядит так:\n",
        "\n",
        "```python\n",
        "class MyNetModel(nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(MyNetModel, self).__init__()\n",
        "        <initialize layers>\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        <apply layers>\n",
        "        return final_output\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "8mHKLbMwx4c5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class CBoWModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.out_layer = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        <apply layers>\n",
        "        return output\n",
        "      \n",
        "model = CBoWModel(vocab_size=len(word2index), embedding_dim=32).cuda()\n",
        "\n",
        "loss_function = <create loss function>\n",
        "optimizer = <create optimizer>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xiIgaofEyyJ1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss_every_nsteps = 1000\n",
        "total_loss = 0\n",
        "start_time = time.time()\n",
        "\n",
        "for step, (batch, labels) in enumerate(make_cbow_batchs_iter(contexts, window_size=2, batch_size=128)):\n",
        "    <copy-paste learning cycle>\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    \n",
        "    if step != 0 and step % loss_every_nsteps == 0:\n",
        "        print(\"Step = {}, Avg Loss = {:.4f}, Time = {:.2f}s\".format(step, total_loss / loss_every_nsteps, \n",
        "                                                                    time.time() - start_time))\n",
        "        total_loss = 0\n",
        "        start_time = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fTEWcyYmvips",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "visualize_embeddings(model.embeddings.weight.data.cpu().numpy(), index2word, 1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CON4VOyG3iET",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Negative Sampling\n",
        "\n",
        "Что сейчас самое тяжелое? Вычисление softmax и применение градиентов ко всем словам в $V$.\n",
        "\n",
        "Один из способов справиться с этим - использовать *Negative Sampling*.\n",
        "\n",
        "По сути, вместо предсказания индекса слова по контексту предсказывается вероятность того, что такое слово $w$ может быть в таком контексте $c$: $P(D=1|w,c)$.\n",
        "\n",
        "Можно использовать обычную сигмоиду для получения данной вероятности: \n",
        "$$P(D=1|w, c) = \\sigma(v_w^T u_c) = \\frac 1 {1 + \\exp(-v^T_w u_c)}.$$\n",
        "\n",
        "Процесс обучения тогда выглядит так: для каждой пары слово и его контекст генерируем набор отрицательных примеров:\n",
        "\n",
        "![Negative Sampling](https://github.com/DanAnastasyev/DeepNLP-Course/raw/master/Week%2003/Images/Negative_Sampling.png =350x)\n",
        "\n",
        "Для CBoW функция потерь будет выглядеть так:\n",
        "$$-\\log \\sigma(v_c^T u_c) - \\sum_{k=1}^K \\log \\sigma(-\\tilde v_k^T u_c),$$\n",
        "где $v_c$ - вектор центрального слова, $u_c$ - вектор контекста (сумма контекстных векторов), $\\tilde v_1, \\ldots, \\tilde v_K$ - сэмплированные негативные примеры.\n",
        "\n",
        "Сравните эту формулу с обычным CBoW:\n",
        "$$-v_c^T u_c + \\log \\sum_{i=1}^{|V|} \\exp(v_i^T u_c).$$\n",
        "\n",
        "Обычно слова сэмплируются из $U^{3/4}$, где $U$ - униграмное распределение, т.е частоты появления слова делённые на суммарое число слов. \n",
        "\n",
        "Частотности мы уже считали: они получаются в `Counter(words)`. Достаточно просто преобразовать их в вероятности и домножить эти вероятности на $\\frac 3 4$. Почему $\\frac 3 4$? Некоторую интуицию можно найти в следующем примере:\n",
        "\n",
        "$$P(\\text{is}) = 0.9, \\ P(\\text{is})^{3/4} = 0.92$$\n",
        "$$P(\\text{Constitution}) = 0.09, \\ P(\\text{Constitution})^{3/4} = 0.16$$\n",
        "$$P(\\text{bombastic}) = 0.01, \\ P(\\text{bombastic})^{3/4} = 0.032$$\n",
        "\n",
        "Вероятность для высокочастотных слов особо не увеличилась (относительно), зато низкочастотные будут выпадать с заметно большей вероятностей.\n",
        "\n",
        "**Задание** Реализуйте свой Negative Sampling.\n",
        "\n",
        "Для начала зададим распределение для сэмплирования:"
      ]
    },
    {
      "metadata": {
        "id": "zcX4vRBLlXy6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "words_sum_count = sum(words_counter.values())\n",
        "word_distribution = np.array([(words_counter[word] / words_sum_count) ** (3 / 4) for word in index2word])\n",
        "# Вообще-то, тут нечестно сделанно, можно лучше\n",
        "word_distribution /= word_distribution.sum()\n",
        "\n",
        "indices = np.arange(len(word_distribution))\n",
        "\n",
        "np.random.choice(indices, p=word_distribution, size=(32, 5))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_o2pzsue16Lu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NegativeSamplingModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.out_layer = nn.Linear(embedding_dim, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, targets, num_samples):\n",
        "        '''\n",
        "        inputs: (batch_size, context_size)\n",
        "        targets: (batch_size)\n",
        "        num_samples: int\n",
        "        '''\n",
        "        \n",
        "        <calculate u_c's>\n",
        "        \n",
        "        <calculate v_c>\n",
        "        \n",
        "        <sample indices>\n",
        "        <calculate negative vectors v'_c>\n",
        "        \n",
        "        <apply F.logsigmoid to v_c * u_c and to -v'_c * u_c>\n",
        "        \n",
        "        <calc result loss>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6wz2iRanqzlq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = NegativeSamplingModel(vocab_size=len(word2index), embedding_dim=32).cuda()\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)  \n",
        "\n",
        "loss_every_nsteps = 1000\n",
        "total_loss = 0\n",
        "start_time = time.time()\n",
        "\n",
        "for step, (batch, labels) in enumerate(make_cbow_batchs_iter(contexts, window_size=2, batch_size=128)):\n",
        "    <copy-paste (mostly) learning cycle>\n",
        "\n",
        "    total_loss += loss.item()\n",
        "    \n",
        "    if step != 0 and step % loss_every_nsteps == 0:\n",
        "        print(\"Step = {}, Avg Loss = {:.4f}, Time = {:.2f}s\".format(step, total_loss / loss_every_nsteps, \n",
        "                                                                    time.time() - start_time))\n",
        "        total_loss = 0\n",
        "        start_time = time.time()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CFik_6djvg3F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "visualize_embeddings(model.embeddings.weight.data.cpu().numpy(), index2word, 1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l4G2X-TTpzwz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Structured Word2Vec\n",
        "\n",
        "**Задание** В статье [Two/Too Simple Adaptations of Word2Vec for Syntax Problems (2015), Ling, Wang, et al.](https://www.aclweb.org/anthology/N/N15/N15-1142.pdf) рассматриваются два варианта улучшения эмбеддингов - *Structured Skip-gram Model* и *Continuous Window Model*:   \n",
        "![](https://github.com/DanAnastasyev/DeepNLP-Course/raw/master/Week%2003/Images/StructuredWord2vec.png =600x)  \n",
        "*From Two/Too Simple Adaptations of Word2Vec for Syntax Problems*\n",
        "\n",
        "Отличие - матрицы для каждого слова контекста учатся свои. Это хорошо на больших корпусах, но на нашем маленьком зайдет не слишком хорошо - многовато параметров придется выучить.\n",
        "\n",
        "Идея этого в том, что порядок слов в предложении очень важен (особенно в английском, на котором они как всегда тестируются). Задавая порядок, они лучше учатся синтаксису.\n",
        "\n",
        "Почитайте статью и попробуйте реализовать один из них."
      ]
    },
    {
      "metadata": {
        "id": "xqDmuu7m_PB5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Дополнительные материалы\n",
        "## Почитать\n",
        "### Блоги\n",
        "[On word embeddings - Part 1, Sebastian Ruder](http://ruder.io/word-embeddings-1/)  \n",
        "[On word embeddings - Part 2: Approximating the Softmax, Sebastian Ruder](http://ruder.io/word-embeddings-softmax/index.html)  \n",
        "[Word2Vec Tutorial - The Skip-Gram Model, Chris McCormick](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)  \n",
        "[Word2Vec Tutorial Part 2 - Negative Sampling, Chris McCormick](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/) \n",
        "\n",
        "### Статьи\n",
        "[Word2vec Parameter Learning Explained (2014), Xin Rong](https://arxiv.org/abs/1411.2738)  \n",
        "[Neural word embedding as implicit matrix factorization (2014), Levy, Omer, and Yoav Goldberg](http://u.cs.biu.ac.il/~nlp/wp-content/uploads/Neural-Word-Embeddings-as-Implicit-Matrix-Factorization-NIPS-2014.pdf)  \n",
        "\n",
        "### Улучшение эмбеддингов\n",
        "[Two/Too Simple Adaptations of Word2Vec for Syntax Problems (2015), Ling, Wang, et al.](https://www.aclweb.org/anthology/N/N15/N15-1142.pdf)  \n",
        "[Not All Neural Embeddings are Born Equal (2014)](https://arxiv.org/pdf/1410.0718.pdf)  \n",
        "[Retrofitting Word Vectors to Semantic Lexicons (2014), M. Faruqui, et al.](https://arxiv.org/pdf/1411.4166.pdf)  \n",
        "[All-but-the-top: Simple and Effective Postprocessing for Word Representations (2017), Mu, et al.](https://arxiv.org/pdf/1702.01417.pdf)  \n",
        "\n",
        "### Эмбеддинги предложений\n",
        "[Skip-Thought Vectors (2015), Kiros, et al.](https://arxiv.org/pdf/1506.06726)  \n",
        "\n",
        "### Backpropagation\n",
        "[Backpropagation, Intuitions, cs231n + next parts in the Module 1](http://cs231n.github.io/optimization-2/)   \n",
        "[Calculus on Computational Graphs: Backpropagation, Christopher Olah](http://colah.github.io/posts/2015-08-Backprop/)\n",
        "\n",
        "## Посмотреть\n",
        "[cs224n \"Lecture 2 - Word Vector Representations: word2vec\"](https://www.youtube.com/watch?v=ERibwqs9p38&index=2&list=PLqdrfNEc5QnuV9RwUAhoJcoQvu4Q46Lja&t=0s)  \n",
        "[cs224n \"Lecture 5 - Backpropagation\"](https://www.youtube.com/watch?v=isPiE-DBagM&index=5&list=PLqdrfNEc5QnuV9RwUAhoJcoQvu4Q46Lja&t=0s)   \n"
      ]
    },
    {
      "metadata": {
        "id": "aM9C1i3Y-6kv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Сдача задания\n",
        "\n",
        "[Сдача](https://goo.gl/forms/rzWjQQsGpqYNz5yt1)  \n",
        "[Опрос](https://goo.gl/forms/as640TWE058bFTpy2)"
      ]
    }
  ]
}