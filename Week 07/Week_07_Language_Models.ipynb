{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 07 - Language Models.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "oTrSUkqEhZzh"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "OE7fXh-OSJYF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 -qq install torch==0.4.1\n",
        "!pip -qq install torchtext==0.3.1\n",
        "!wget -qq --no-check-certificate 'https://drive.google.com/uc?export=download&id=1Pq4aklVdj-sOnQw68e1ZZ_ImMiC8IR1V' -O tweets.csv.zip\n",
        "!wget -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1ji7dhr9FojPeV51dDlKRERIqr3vdZfhu\" -O surnames.txt\n",
        "!unzip tweets.csv.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uhvfH55PUJ8K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "    DEVICE = torch.device('cuda')\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "    DEVICE = torch.device('cpu')\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jVcnkGDgxfNx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Языковые модели"
      ]
    },
    {
      "metadata": {
        "id": "8Kjg1Z3xxmEP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*Языковая модель* - это штука, которая умеет оценивать вероятности встретить последовательность слов $w_1, \\ldots, w_n$:   \n",
        "$$\\mathbf{P}(w_1, \\ldots, w_n) = \\prod_k \\mathbf{P}(w_k|w_{k-1}, \\ldots, w_{1}).$$\n",
        "\n",
        "Интерпретируемы и интересны тут именно условные вероятности - какое слово языковая модель ожидает вслед за данными. У нас у всех такая языковая модель есть, так-то. Например, в таком контексте\n",
        "\n",
        "![](https://hsto.org/web/956/239/601/95623960157b4e15a1b3f599aed62ed2.png =x170)\n",
        "\n",
        "моя языковая модель говорит - после *честных* навряд ли пойдёт *мой*. А вот *и* или, конечно, *правил* - очень даже.\n",
        "\n",
        "А задача такая: научиться генерировать политические твиты по образу и подобию `Russian Troll Tweets`. Датасет взят отсюда: https://www.kaggle.com/vikasg/russian-troll-tweets"
      ]
    },
    {
      "metadata": {
        "id": "JpjfUoN4_WY7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv('tweets.csv')\n",
        "\n",
        "data.text.sample(15).tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WAQ4d__2_sAz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Да, результаты будут упороты, сразу предупреждаю."
      ]
    },
    {
      "metadata": {
        "id": "7Qvqidof7Fsi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Чтение данных"
      ]
    },
    {
      "metadata": {
        "id": "OSu56oDX-KY5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Кого-нибудь уже достало писать все эти построения батчей, словари - вот это всё? Лично меня - да!\n",
        "\n",
        "В pytorch есть специальный класс для генерации батчей - `Dataset`. Вместо того, чтобы писать функцию типа `iterate_batches`, можно отнаследовать от него и переопределить методы `__len__` и `__getitem__`... и реализовать в них почти всё то, что было в `iterate_batches`. Пока не впечатляет, да?\n",
        "\n",
        "Ещё там есть `DataLoader`, умеющий работать с датасетом. Он позволяет делать shuffle батчей и генерацию их в отдельных процессах - это особенно важно, когда генерация батча - долгая операция. Например, в картинках. Почитать про это всё можно здесь: [Data Loading and Processing Tutorial](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html).\n",
        "\n",
        "Но пока что всё равно не особо круто, мне кажется. Интересно другое - у pytorch в репозитории живет отдельная библиотечка - [torchtext](https://github.com/pytorch/text). Вот она уже даст нам специальные реализации `Dataset` для работы с текстом и всякие тулзы, делающие жизнь чуточку проще.\n",
        "\n",
        "Библиотеке, на мой взгляд, недостает туториалов, в которых бы показывалось, как с ней работать - но можно читать исходный код, он приятный.\n",
        "\n",
        "План такой: построить класс `torchtext.data.Dataset`, для него создать итератор, и учить модель.\n",
        "\n",
        "Данный датасет инициализируется двумя параметрами:\n",
        "```\n",
        "            examples: List of Examples.\n",
        "            fields (List(tuple(str, Field))): The Fields to use in this tuple. The\n",
        "                string is a field name, and the Field is the associated field.\n",
        "```\n",
        "Разберемся сначала со вторым.\n",
        "\n",
        "`Field` - это такая мета-информация для датасета + обработчик сэмплов.  \n",
        "\n",
        "Он имеет кучу параметров, на которые проще посмотреть [здесь](https://github.com/pytorch/text/blob/master/torchtext/data/field.py). Если коротко, то он может предобрабатывать (например, токенизировать) предложения, строить словарь (отображение из слова в индекс), строить батчи - добавлять паддинги и конвертировать в тензоры. Что ещё нужно в жизни?\n",
        "\n",
        "Мы будем делать character-level языковую модель, поэтому токенизация для нас - превращение строки в набор символов. Попросим также добавлять в начало и конец спец-символы `<s>` и `</s>`."
      ]
    },
    {
      "metadata": {
        "id": "ilAMVxA8Xy4L",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchtext.data import Field\n",
        "\n",
        "text_field = Field(init_token='<s>', eos_token='</s>', lower=True, tokenize=lambda line: list(line))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L_i0Z6JhF0rA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Препроцессинг будет выглядеть так:"
      ]
    },
    {
      "metadata": {
        "id": "B-8IPlPHFyKa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text_field.preprocess(data.text.iloc[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "19NFhTSNF1_1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Сконвертируем всё и посмотрим на распределение длин:"
      ]
    },
    {
      "metadata": {
        "id": "wz1QnivMBmU3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "data['text'] = data['text'].fillna('')\n",
        "lines = data.apply(lambda row: text_field.preprocess(row['text']), axis=1).tolist()\n",
        "\n",
        "lengths = [len(line) for line in lines]\n",
        "\n",
        "plt.hist(lengths, bins=30)[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dE9rPW9UHE7d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Отсечем слишком короткие строки и преобразуем оставшиеся в `Example`'ы:"
      ]
    },
    {
      "metadata": {
        "id": "jfTlpBxODBg8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchtext.data import Example\n",
        "\n",
        "lines = [line for line in lines if len(line) >= 50]\n",
        "\n",
        "fields = [('text', text_field)]\n",
        "examples = [Example.fromlist([line], fields) for line in lines]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7z1wPlz_HeEP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "По `Example` можно получить обратно все поля, которые мы туда запихнули. Например, сейчас мы создали одно поле `text`:"
      ]
    },
    {
      "metadata": {
        "id": "iGMRSuk_HYCm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "examples[0].text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yef1bv2MQcEA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Построим, наконец, датасет:"
      ]
    },
    {
      "metadata": {
        "id": "gSccEmVIHAaQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchtext.data import Dataset\n",
        "\n",
        "dataset = Dataset(examples, fields)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vEe5YXIpRCYD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Датасет можно разбить на части:"
      ]
    },
    {
      "metadata": {
        "id": "21whmJDFRBV1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dataset, test_dataset = dataset.split(split_ratio=0.75)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "14CyhugSQsOf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "По нему можно построить словарь:"
      ]
    },
    {
      "metadata": {
        "id": "NQs3jbhyQkJD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text_field.build_vocab(train_dataset, min_freq=30)\n",
        "\n",
        "print('Vocab size =', len(text_field.vocab))\n",
        "print(text_field.vocab.itos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-_EAdgsWRTzj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Наконец, по нему можно итерироваться:"
      ]
    },
    {
      "metadata": {
        "id": "qaEMoxdVG98p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchtext.data import BucketIterator\n",
        "\n",
        "train_iter, test_iter = BucketIterator.splits(datasets=(train_dataset, test_dataset), batch_sizes=(32, 128), \n",
        "                                              shuffle=True, device=DEVICE, sort=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RMG4L1-5RXnb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_iter))\n",
        "\n",
        "batch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7ZZgplOkReeq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch.text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oTrSUkqEhZzh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Перплексия"
      ]
    },
    {
      "metadata": {
        "id": "gqc9HpTM-FwD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Нашу задачу, как всегда, нужно начинать с двух вопросов - какую метрику оптимизируем и какой бейзлайн.\n",
        "\n",
        "С метрикой всё просто - мы хотим, чтобы модель как можно лучше умела приближать распределение слов языка. Всего языка у нас нету, поэтому обойдёмся тестовой выборкой.\n",
        "\n",
        "На ней можно посчитать кросс-энтропийные потери: \n",
        "$$H(w_1, \\ldots, w_n) = - \\frac 1n \\sum_k \\log\\mathbf{P}(w_k | w_{k-1}, \\ldots, w_1).$$\n",
        "\n",
        "Здесь вероятность $\\mathbf{P}$ - это вероятность, оцененная нашей языковой моделью. Идеальная модель давала бы вероятность равную 1 для слов в тексте и потери были бы нулевыми - хотя это, конечно, невозможно, даже вы же не можете предсказать следующее слово, что уж про бездушную машину говорить.\n",
        "\n",
        "Таким образом, всё как всегда - оптимизируем кросс-энтропию и стремимся сделать её как можно ниже.\n",
        "\n",
        "Ну, почти всё. Ещё есть отдельная метрика для языковых моделей - *перплексия*. Это просто возведенные в экспоненту кросс-энтропийные потери:\n",
        "\n",
        "$$PP(w_1, \\ldots, w_n) = e^{H(w_1, \\ldots, w_n)} = e^{- \\frac 1n \\sum_k \\log\\mathbf{P}(w_k | w_{k-1}, \\ldots, w_1)} = \\left(\\mathbf{P}(w_1, \\ldots, w_n) \\right)^{-\\frac 1n}.$$\n",
        "\n",
        "У её измерения есть некоторый сакральный смысл кроме банальной интепретируемости: представим модель, предсказывающую слова из словаря равновероятно вне зависимости от контекста. Для неё $\\mathbf{P}(w) = \\frac 1 N$, где $N$ — размер словаря, а перплексия будет равна размеру словаря — $N$. Конечно, это совершенно глупая модель, но оглядываясь на неё, можно трактовать перплексию реальных моделей как уровень неоднозначности генерации слова.\n",
        "\n",
        "Скажем, в модели с перплексией 100 выбор следующего слова также неоднозначен, как выбор из равномерного распределения среди 100 слов. И если такой перплексии удалось достичь на словаре в 100 000, получается, что удалось сократить эту неоднозначность на три порядка по сравнению с тупым рандомом."
      ]
    },
    {
      "metadata": {
        "id": "xW8I0lKv9y1H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Бейзлайн"
      ]
    },
    {
      "metadata": {
        "id": "7wInBuBn-DIf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Вообще, бейзлайн тут тоже очень простой. Мы, на самом деле, даже смотрели его на курсе концепций: [N-граммная языковая модель](https://colab.research.google.com/drive/1lz9vO6Ue5zOiowEx0-koXNiejBrrnbj0). Можно подсчитывать вероятности N-грамм слов по частотностям их появления в обучающем корпусе. А дальше использовать аппроксимацию $\\mathbf{P}(w_k|w_1, \\ldots, w_{k-1}) \\approx \\mathbf{P}(w_k|w_{k-1}, \\ldots, w_{k-N + 1})$.\n",
        "\n",
        "Применим лучше сеточки для реализации того же.\n",
        "\n",
        "![](https://image.ibb.co/buMnLf/2018-10-22-00-22-56.png =x450)  \n",
        "*From cs224n, Lecture 8 [pdf](http://web.stanford.edu/class/cs224n/lectures/lecture8.pdf)*\n",
        "\n",
        "На вход приходит последовательность слов, они эмбеддятся, а дальше с помощью выходного слоя считается наиболее вероятное следующее слово.\n",
        "\n",
        "Стоп... Но мы же уже реализовывали такое! В Word2vec CBoW модели мы по контексту предсказывали центральное слово - единственное отличие в том, что теперь мы имеем только левый контекст. Значит, всё, идём к следующей модели?\n",
        "\n",
        "Нет! Тут ещё есть с чем развлечься. В Word2vec мы формировали батчи таким образом:\n",
        "![](https://image.ibb.co/bs3wgV/training-data.png =x350)  \n",
        "*From [Word2Vec Tutorial - The Skip-Gram Model, Chris McCormic](http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/)*\n",
        "\n",
        "То есть нарезали из текста набор пар <контекст, слово> (и как-то их использовали в зависимости от метода).\n",
        "\n",
        "Это нерационально - каждое слово повторяется много раз. Но можно использовать сверточные сети - они за нас применят операцию умножения на $W$ к каждому окну. В результате размер входного батча будет сильно меньше.\n",
        "\n",
        "Чтобы правильно всё обработать, нужно добавить паддинг в начало последовательности размером `window_size - 1` - тогда первое слово будет предсказываться по `<pad>...<pad><s>`.\n",
        "\n",
        "**Задание** Реализуйте языковую модель с фиксированным окном."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "A-tn_Gmi3pU0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ConvLM(nn.Module):\n",
        "    def __init__(self, vocab_size, window_size=5, emb_dim=16, filters_count=128):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._window_size = window_size\n",
        "        \n",
        "        <init layers>\n",
        "        \n",
        "    def forward(self, inputs):\n",
        "        <apply>\n",
        "        \n",
        "        return output, None  # hacky way to use training cycle for RNN and Conv simultaneously"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bjpOLKBH5yS5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Проверим, что оно работает:"
      ]
    },
    {
      "metadata": {
        "id": "ks_RTZ14nMRz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = ConvLM(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n",
        "\n",
        "model(batch.text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lb_2VTBW5v_7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Реализуйте функцию для сэмплирования последовательности из языковой модели."
      ]
    },
    {
      "metadata": {
        "id": "0oUg0BjV2JjE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sample(probs, temp):\n",
        "    probs = F.log_softmax(probs.squeeze(), dim=0)\n",
        "    probs = (probs / temp).exp()\n",
        "    probs /= probs.sum()\n",
        "    probs = probs.cpu().numpy()\n",
        "\n",
        "    return np.random.choice(np.arange(len(probs)), p=probs)\n",
        "\n",
        "\n",
        "def generate(model, temp=0.7):\n",
        "    model.eval()\n",
        "    \n",
        "    history = [train_dataset.fields['text'].vocab.stoi['<s>']]\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for _ in range(150):\n",
        "            <sample next character and print it (use end='' in print function)>\n",
        "\n",
        "generate(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CXuN871a852l",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Мы до сих пор не задали ни какой target. А предсказывать нам будет нужно следующие слова - то есть просто сдвинутый на 1 входной тензор. Реализуйте построение target'а и подсчет потерь."
      ]
    },
    {
      "metadata": {
        "id": "CGLkcXARjhTM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data_iter, unk_idx, pad_idx, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = len(data_iter)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, batch in enumerate(data_iter):                \n",
        "                logits, _ = model(batch.text)\n",
        "\n",
        "                <implement loss calc>\n",
        "                \n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
        "                    optimizer.step()\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), \n",
        "                                                                                         math.exp(loss.item())))\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(\n",
        "                name, epoch_loss / batches_count, math.exp(epoch_loss / batches_count))\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_iter, epochs_count=1, unk_idx=0, pad_idx=1, val_iter=None):\n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss = do_epoch(model, criterion, train_iter, unk_idx, pad_idx, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_iter is None:\n",
        "            val_loss = do_epoch(model, criterion, val_iter, unk_idx, pad_idx, None, name_prefix + '  Val:')\n",
        "\n",
        "        generate(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LIj0Lcdh9UJy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = ConvLM(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n",
        "\n",
        "pad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\n",
        "unk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\n",
        "criterion = nn.CrossEntropyLoss(reduction='none').to(DEVICE)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_iter, epochs_count=30, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FycAd6MWMvYy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Чтобы отучить модель сэмплировать `<unk>` можно явным образом запрещать это в сэплирующей функции - а можно просто не учить ее на них. Реализуйте маскинг по одновременно и паддингам, и неизвестным словам."
      ]
    },
    {
      "metadata": {
        "id": "rQJKn1Uw94_0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Рекуррентная языковая модель"
      ]
    },
    {
      "metadata": {
        "id": "HeSojPwh_ZSS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Очевидно, хочется использовать не фиксированное окно истории, а всю информацию об уже сгенерированном. Как минимум, хочется знать, когда у нас лимит символов в твите подошел. \n",
        "Для этого используют рекуррентные языковые модели:\n",
        "\n",
        "![](https://hsto.org/web/dc1/7c2/c4e/dc17c2c4e9ac434eb5346ada2c412c9a.png =x250)\n",
        "\n",
        "Сети на вход передается предыдующий токен, а также предыдущее состояние RNN. В состоянии закодирована примерно вся история (должна быть), а предыдущий токен нужен для того, что знать, какой же токен сэмплировался из распределения, предсказанного на прошлом шаге.\n",
        "\n",
        "**Задание** Мы уже несколько раз так делали - реализуйте снова сеть, которая будет заниматься языковым моделированием."
      ]
    },
    {
      "metadata": {
        "id": "x8ndCRZLl4ZZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class RnnLM(nn.Module):\n",
        "    def __init__(self, vocab_size, emb_dim=16, lstm_hidden_dim=128, num_layers=1):\n",
        "        super().__init__()\n",
        "\n",
        "        self._emb = nn.Embedding(vocab_size, emb_dim)\n",
        "        self._rnn = nn.LSTM(input_size=emb_dim, hidden_size=lstm_hidden_dim)\n",
        "        self._out_layer = nn.Linear(lstm_hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, hidden=None):\n",
        "        <implement me>\n",
        "        return output, hidden"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H3MjLgDKBNsD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Реализуйте функцию для сэмплирования предложений из модели."
      ]
    },
    {
      "metadata": {
        "id": "ZJSXu_Pr_kYL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate(model, temp=0.8):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        prev_token = train_iter.dataset.fields['text'].vocab.stoi['<s>']\n",
        "        end_token = train_iter.dataset.fields['text'].vocab.stoi['</s>']\n",
        "        \n",
        "        hidden = None\n",
        "        for _ in range(150):\n",
        "            <print sampled character>\n",
        "\n",
        "generate(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cibfrMxo_Gjg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = RnnLM(vocab_size=len(train_iter.dataset.fields['text'].vocab)).to(DEVICE)\n",
        "\n",
        "pad_idx = train_iter.dataset.fields['text'].vocab.stoi['<pad>']\n",
        "unk_idx = train_iter.dataset.fields['text'].vocab.stoi['<unk>']\n",
        "criterion = nn.CrossEntropyLoss(reduction='none').to(DEVICE)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_iter, epochs_count=30, unk_idx=unk_idx, pad_idx=pad_idx, val_iter=test_iter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8cCcKrWjBzCp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Улучшения модели\n",
        "\n",
        "### Оптимизатор\n",
        "\n",
        "Мы использовали только `Adam` до сих пор. Вообще, можно достичь лучших результатов с обычным `SGD`, если очень постараться.\n",
        " \n",
        "**Задание** Замените оптимизатор на `optim.SGD(model.parameters(), lr=20., weight_decay=1e-6)`. Например. Или другими параметрами на выбор.\n",
        "\n",
        "### Dropout\n",
        "\n",
        "Вспомним, что такое dropout.\n",
        "\n",
        "По сути это умножение случайно сгенерированной маски из нулей и единиц на входной вектор (+ нормировка).\n",
        "\n",
        "Например, для слоя Dropout(p):\n",
        "\n",
        "$$m = \\frac1{1-p} \\cdot \\text{Bernouli}(1 - p)$$\n",
        "$$\\tilde h = m \\odot h $$\n",
        "\n",
        "В рекуррентных сетях долго не могли прикрутить dropout. Делать это пытались, генерируя случайную маску:   \n",
        "![A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](https://cdn-images-1.medium.com/max/800/1*g4Q37g7mlizEty7J1b64uw.png =x300)  \n",
        "from [A Theoretically Grounded Application of Dropout in Recurrent Neural Networks](https://arxiv.org/abs/1512.05287)\n",
        "\n",
        "Оказалось, правильнее делать маску фиксированную: для каждого шага должны зануляться одни и те же элементы.\n",
        "\n",
        "Для pytorch нет нормального встроенного variational dropout в LSTM. Зато есть [AWD-LSTM](https://github.com/salesforce/awd-lstm-lm).\n",
        "\n",
        "Советую посмотреть обзор разных способов применения dropout'а в рекуррентных сетях: [Dropout in Recurrent Networks — Part 1](https://becominghuman.ai/learning-note-dropout-in-recurrent-networks-part-1-57a9c19a2307) (в конце - ссылки на Part 2 и 3).\n",
        "\n",
        "**Задание** Реализуйте вариационный dropout. Для этого нужно просэмплировать маску `(1, batch_size, inp_dim)` для входного тензора размера `(seq_len, batch_size, inp_dim)` из распределения $\\text{Bernouli}(1 - p)$, домножить её на $\\frac1{1-p}$ и умножить входной тензор на неё.\n",
        "\n",
        "Благодаря broadcasting каждый timestamp из входного тензора домножится на одну и ту же маску - и должно быть счастье.\n",
        "\n",
        "Хотя лучше сравнить с обычным `nn.Dropout`, вдруг разница не будет заметна."
      ]
    },
    {
      "metadata": {
        "id": "aDv4nutY-WOw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LockedDropout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, inputs, dropout=0.5):\n",
        "        if not self.training or not dropout:\n",
        "            return inputs\n",
        "        \n",
        "        <implement me>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9m-InMeoIiCA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Условная генерация"
      ]
    },
    {
      "metadata": {
        "id": "J7aB2_YxIl-c",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Мы уже классифицировали фамилии по языкам. Научимся теперь генерировать фамилию при заданном языке.\n",
        "\n",
        "Воспользуемся наследником `Dataset` - `TabularDataset`:"
      ]
    },
    {
      "metadata": {
        "id": "Wa5benKoJMfc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchtext.data import TabularDataset\n",
        "\n",
        "name_field = Field(init_token='<s>', eos_token='</s>', lower=True, tokenize=lambda line: list(line))\n",
        "lang_field = Field(sequential=False)\n",
        "\n",
        "dataset = TabularDataset(\n",
        "    path='surnames.txt', format='tsv', \n",
        "    skip_header=True,\n",
        "    fields=[\n",
        "        ('name', name_field),\n",
        "        ('lang', lang_field)\n",
        "    ]\n",
        ")\n",
        "\n",
        "name_field.build_vocab(dataset)\n",
        "lang_field.build_vocab(dataset)\n",
        "\n",
        "print(name_field.vocab.itos)\n",
        "print(lang_field.vocab.itos)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qp3SZHAsK85C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Разобьем датасет:"
      ]
    },
    {
      "metadata": {
        "id": "kh-KKh08J5Oq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_dataset, val_dataset = dataset.split(split_ratio=0.25, stratified=True, strata_field='lang')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nIzaiUKDK_PG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Сделать языковую модель, которая принимает как предыдующий сгенерированный символ, так и индекс языка, к которому это слово относится. Строить эмбеддинги для символа и для языка, конкатенировать их - а дальше всё то же самое.\n",
        "\n",
        "Нужно обучить эту модель и написать функцию-генератор фамилий при заданном языке."
      ]
    },
    {
      "metadata": {
        "id": "s6LnEoU9LNlZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9VfdL29AELhu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# In the wild"
      ]
    },
    {
      "metadata": {
        "id": "GDqxGVo5EOfb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Применим свои знания к боевой задаче: [Kaggle Toxic Comment Classification Challenge](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge/).\n",
        "\n",
        "Она про классификацию сообщений по нескольким категориям. Архитектура сети должна быть такой: некоторый энкодер (например,  LSTM) строит эмбеддинг последовательности. Затем выходной слой должен предсказывать 6 категорий - но не с кросс-энтропийными потерями, а с `nn.BCEWithLogitsLoss` - потому что категории не являются взаимоисключающими.\n",
        "\n",
        "Совет: разберитесь с токенизацией, которую умеет `Field`. Скачайте предобученные словные эмбеддинги, как мы делали. Постройте сеть и напишите цикл обучения для неё.\n",
        "\n",
        "**Задание** Скачать данные с kaggle, потренировать что-нибудь и сделать посылку."
      ]
    },
    {
      "metadata": {
        "id": "8obdAs_E0zRb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Дополнительные материалы\n",
        "\n",
        "## Блоги\n",
        "\n",
        "[A Friendly Introduction to Cross-Entropy Loss, Rob DiPietro](https://rdipietro.github.io/friendly-intro-to-cross-entropy-loss/)\n",
        "\n",
        "[A Tutorial on Torchtext, Allen Nie](http://anie.me/On-Torchtext/)\n",
        "\n",
        "[Dropout in Recurrent Networks, Ceshine Lee](https://becominghuman.ai/learning-note-dropout-in-recurrent-networks-part-1-57a9c19a2307)\n",
        "\n",
        "[The Unreasonable Effectiveness of Recurrent Neural Networks, Andrej Karpathy](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
        "\n",
        "[The unreasonable effectiveness of Character-level Language Models, Yoav Goldberg](http://nbviewer.jupyter.org/gist/yoavg/d76121dfde2618422139)\n",
        "\n",
        "[Unsupervised Sentiment Neuron, OpenAI](https://blog.openai.com/unsupervised-sentiment-neuron/)\n",
        "\n",
        "[Как научить свою нейросеть генерировать стихи](https://habr.com/post/334046/)\n",
        "\n",
        "## Видео\n",
        "[cs224n, \"Lecture 8: Recurrent Neural Networks and Language Models\"](https://www.youtube.com/watch?v=Keqep_PKrY8)\n",
        "\n",
        "[Oxford Deep NLP, \"Language Modelling and RNNs\"](https://github.com/oxford-cs-deepnlp-2017/lectures#5-lecture-3---language-modelling-and-rnns-part-1-phil-blunsom)"
      ]
    },
    {
      "metadata": {
        "id": "WJVDoh5MLcdB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Сдача\n",
        "\n",
        "[Опрос для сдачи](https://goo.gl/forms/8bjGv7LLWUrwOUrt2)\n",
        "\n",
        "[Feedback](https://goo.gl/forms/PR76tYmvzMugIFID2)"
      ]
    }
  ]
}