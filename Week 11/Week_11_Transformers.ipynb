{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 11 - Transformers.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "OE7fXh-OSJYF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip3 -qq install torch==0.4.1\n",
        "!pip -qq install torchtext==0.3.1\n",
        "!pip install sacremoses==0.0.5\n",
        "!wget -O news.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1hIVVpBqM6VU4n3ERkKq4tFaH4sKN0Hab\"\n",
        "!unzip news.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uhvfH55PUJ8K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "    DEVICE = torch.device('cuda')\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "    DEVICE = torch.device('cpu')\n",
        "\n",
        "np.random.seed(42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "txWqIO_74A4s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Abstactive Summarization"
      ]
    },
    {
      "metadata": {
        "id": "gJ7JQJMO2R7z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Задача - по тексту сгенерировать выдержку из него.\n",
        "\n",
        "Например, попробуем по новостям генерировать заголовки:"
      ]
    },
    {
      "metadata": {
        "id": "NzNGUFOcXMUs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!shuf -n 10 news.csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QOVlO5_Qlg5y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Токенизируем их. Будем использовать единый словарь для текста и заголовков."
      ]
    },
    {
      "metadata": {
        "id": "fsOvtO0fpCHa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from torchtext.data import Field, Example, Dataset, BucketIterator\n",
        "\n",
        "BOS_TOKEN = '<s>'\n",
        "EOS_TOKEN = '</s>'\n",
        "\n",
        "word_field = Field(tokenize='moses', init_token=BOS_TOKEN, eos_token=EOS_TOKEN, lower=True)\n",
        "fields = [('source', word_field), ('target', word_field)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VO-gix7yoBjg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "data = pd.read_csv('news.csv', delimiter=',')\n",
        "\n",
        "examples = []\n",
        "for _, row in tqdm(data.iterrows(), total=len(data)):\n",
        "    source_text = word_field.preprocess(row.text)\n",
        "    target_text = word_field.preprocess(row.title)\n",
        "    examples.append(Example.fromlist([source_text, target_text], fields))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A8uCsMEglm6V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Построим датасеты:"
      ]
    },
    {
      "metadata": {
        "id": "ZOBgLAgVTrk1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "dataset = Dataset(examples, fields)\n",
        "\n",
        "train_dataset, test_dataset = dataset.split(split_ratio=0.85)\n",
        "\n",
        "print('Train size =', len(train_dataset))\n",
        "print('Test size =', len(test_dataset))\n",
        "\n",
        "word_field.build_vocab(train_dataset, min_freq=7)\n",
        "print('Vocab size =', len(word_field.vocab))\n",
        "\n",
        "train_iter, test_iter = BucketIterator.splits(\n",
        "    datasets=(train_dataset, test_dataset), batch_sizes=(16, 32), shuffle=True, device=DEVICE, sort=False\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tHQGgoit2ljv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Seq2seq for Abstractive Summarization\n",
        "\n",
        "Вообще задача не сильно отличается от машинного перевода:\n",
        "\n",
        "![](https://image.ibb.co/jAf3S0/2018-11-20-9-42-17.png)\n",
        "*From [Get To The Point: Summarization with Pointer-Generator Networks](https://arxiv.org/pdf/1704.04368.pdf)*\n",
        "\n",
        "Тут на каждом шаге декодер подглядывает на все токены - точнее, их эмбеддинги после BiRNN.\n",
        "\n",
        "Возникает вопрос - а зачем вообще RNN, если потом все равно будем смотреть на всё."
      ]
    },
    {
      "metadata": {
        "id": "8FYJe2CA8GcY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Transformer\n",
        "\n",
        "Из этой идеи - отказ от RNN - и получился Transformer.\n",
        "\n",
        "![](https://hsto.org/webt/59/f0/44/59f04410c0e56192990801.png =x600)  \n",
        "*From Attention is all you need*\n",
        "\n",
        "Как в случае с RNN мы на каждом шаге применяем одну и ту же операцию (ячейку LSTM) к текущему входу, так и здесь - только теперь связей между timestamp'ами нет и можно обрабатывать их почти параллельно.\n",
        "\n",
        "*Код дальше очень сильно опирается на шикарную статью [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html).*"
      ]
    },
    {
      "metadata": {
        "id": "env8rDS_dM86",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Encoder\n",
        "\n",
        "Начнем с энкодера:\n",
        "\n",
        "![](http://jalammar.github.io/images/t/transformer_resideual_layer_norm.png =x400)  \n",
        "*From [Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)*\n",
        "\n",
        "Он представляет из себя последовательность одинаковых блоков с self-attention + полносвязными слоями.\n",
        "\n",
        "Можно представить, что это - ячейка LSTM: она тоже применяется к каждому входу с одинаковыми весами. Разница основная в отсутствии рекуррентных связей: за счет этого энкодер может применяться одновременно ко всем входам батча."
      ]
    },
    {
      "metadata": {
        "id": "rXjWcnpCJY92",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Positional Encoding\n",
        "\n",
        "Нужно как-то кодировать информацию о том, в каком месте в предложении стоит токен. Чуваки предложили делать так:\n",
        "$$PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\\text{model}}})$$\n",
        "$$PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\\text{model}}})$$\n",
        "\n",
        "где $(pos, i)$ - позиция в предложении и индекс в скрытом векторе размерности до $d_{model}$."
      ]
    },
    {
      "metadata": {
        "id": "zI2rMiZhJcKX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math \n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:, :x.size(1)]\n",
        "        return self.dropout(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XEL9VppyKCBz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "pe = PositionalEncoding(20, 0)\n",
        "y = pe(torch.zeros(1, 100, 20))\n",
        "plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
        "plt.legend([\"dim %d\"%p for p in [4,5,6,7]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a7OeqTcOdgud",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "В итоге эмбеддинги токена получается как сумма обычного эмбеддинга и эмбеддинга позиции:  \n",
        "![](http://jalammar.github.io/images/t/transformer_positional_encoding_vectors.png =x500)  \n",
        "*From [Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)*"
      ]
    },
    {
      "metadata": {
        "id": "9rvG4ZQ_icKH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Residual Connection\n",
        "\n",
        "Разберем блок энкодера - повторяющейся N раз комбинации операций на первом рисунке.\n",
        "\n",
        "Самое простое здесь - residual connection. Вместо к выходу произвольной функции $F$ прибавляется её вход\n",
        "$$y = F(x) \\quad \\to \\quad y = F(x) + x$$\n",
        "\n",
        "Идея в том, что обычные сети сложно делать слишком глубокими - градиенты затухают. А через этот residual вход $x$ градиентам течь ничего не стоит. В итоге в картинках благодаря таким блокам получилось настакать дофига слоев и улучшить качество (см. ResNet).\n",
        "\n",
        "Ничего не мешает нам поступить также."
      ]
    },
    {
      "metadata": {
        "id": "7QLCqNeEjKpD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, size, dropout_rate):\n",
        "        super().__init__()\n",
        "        self._norm = LayerNorm(size)\n",
        "        self._dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, inputs, sublayer):\n",
        "        return inputs + self._dropout(sublayer(self._norm(inputs)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EHUqCsifmd5e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Layer Norm\n",
        "\n",
        "Дополнительно применяется нормализация LayerNorm. \n",
        "\n",
        "**Batch normalization**  \n",
        "Мы вообще не разбирали, но BatchNorm работает так:\n",
        "$$\\mu_j = \\frac{1}{m}\\sum_{i=1}^{m}x_{ij} \\\\    \\sigma_j^2 = \\frac{1}{m}\\sum_{i=1}^{m}(x_{ij} - \\mu_j)^2 \\\\    \\hat{x}_{ij} = \\frac{x_{ij} - \\mu_j}{\\sqrt{\\sigma_j^2 + \\epsilon}}$$\n",
        "$$y_{ij} = \\gamma \\ \\hat{x}_{ij} + \\beta$$\n",
        "\n",
        "На каждом батче эти $\\mu$ и $\\sigma$ пересчитываются, обновляя статистики. На инференсе используются накопленные статистики.\n",
        "\n",
        "Основной его недостаток - он плохо работает с рекуррентными сетями. Чтобы побороть это придумали:\n",
        "\n",
        "**Layer normalization**  \n",
        "А сейчас мы будем пользоваться немного другими формулами:\n",
        "$$\\mu_i = \\frac{1}{m}\\sum_{j=1}^{m}x_{ij} \\\\    \\sigma_i^2 = \\frac{1}{m}\\sum_{j=1}^{m}(x_{ij} - \\mu_i)^2 \\\\    \\hat{x}_{ij} = \\frac{x_{ij} - \\mu_i}{\\sqrt{\\sigma_i^2 + \\epsilon}}$$\n",
        "$$y_{ij} = \\gamma \\ \\hat{x}_{ij} + \\beta$$\n",
        "\n",
        "Разницу с ходу не видно, но она есть:\n",
        "![](https://image.ibb.co/hjtuX0/layernorm.png)  \n",
        "*From [Weight Normalization and Layer Normalization Explained ](http://mlexplained.com/2018/01/13/weight-normalization-and-layer-normalization-explained-normalization-in-deep-learning-part-2/)*\n",
        "\n",
        "Если в BatchNorm статистики считаются для каждой фичи усреднением по батчу, то теперь - для каждого входа усредением по фичам."
      ]
    },
    {
      "metadata": {
        "id": "Q5XLZZ3zrK24",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, features, eps=1e-6):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._gamma = nn.Parameter(torch.ones(features))\n",
        "        self._beta = nn.Parameter(torch.zeros(features))\n",
        "        self._eps = eps\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        <calc it>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WrWvtymp2G8o",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Attention\n",
        "\n",
        "Весь Transformer опирается на идею self-attention. Выглядит это так:\n",
        "\n",
        "![](http://jalammar.github.io/images/t/transformer_self-attention_visualization.png)  \n",
        "*From [Tensor2Tensor Tutorial](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)*\n",
        "\n",
        "Эмбеддинг слова *it* строится как комбинация всех эмбеддингов предложения.\n",
        "\n",
        "В статье придумали делать такой аттеншен:\n",
        "\n",
        "$$\\mathrm{Attention}(Q, K, V) = \\mathrm{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
        "\n",
        "Это примерно как dot-attention на прошлом занятии: запрос (**Q**uery) умножается на ключи (**K**ey) скалярно, затем берется софтмакс - получаются оценки того, насколько интересны разные таймстемпы из значений (**V**alue). \n",
        "\n",
        "Например, $\\mathrm{emb}(\\text{it}) = \\mathrm{Attention}(\\text{it}, \\ldots\\text{because it was too tired}, \\ldots\\text{because it was too tired})$.\n",
        "\n",
        "Только теперь ещё с параметром $\\frac{1}{\\sqrt{d_k}}$, где $d_k$ - это размерность ключа. Утверждается, это работает лучше при больших размерностях ключа $d_k$."
      ]
    },
    {
      "metadata": {
        "id": "2ApuVJZn5i4R",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, dropout_rate):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._dropout = nn.Dropout(dropout_rate)\n",
        "        \n",
        "    def forward(self, query, key, value, mask):\n",
        "        <calc it>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uZ-xQbgM6MNl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Multi-Head Attention\n",
        "\n",
        "![](https://hsto.org/webt/59/f0/44/59f0440f1109b864893781.png)\n",
        "\n",
        "Важная идея, почему attention (и, главное, self-attention) заработал - использование нескольких голов (multi-head).\n",
        "\n",
        "Вообще, когда мы делаем attention - мы определяем похожесть ключа и запроса. Многоголовость помогает (должна) определять эту похожесть по разным критериям - синтаксически, семантически и т.д.\n",
        "\n",
        "Например, на картинке используется две головы и одна голова смотрит на *the animal* при генерации *it*, вторая - на *tired*:\n",
        "\n",
        "![](http://jalammar.github.io/images/t/transformer_self-attention_visualization_2.png)  \n",
        "*From [Tensor2Tensor Tutorial](https://colab.research.google.com/github/tensorflow/tensor2tensor/blob/master/tensor2tensor/notebooks/hello_t2t.ipynb)*\n",
        "\n",
        "Применяется это таким образом:\n",
        "\n",
        "$$\\mathrm{MultiHead}(Q, K, V) = \\mathrm{Concat}(\\mathrm{head_1}, ...,\n",
        "\\mathrm{head_h})W^O    \\\\\n",
        "    \\mathrm{head_i} = \\mathrm{Attention}(QW^Q_i, KW^K_i, VW^V_i)$$\n",
        "    \n",
        "где $W^Q_i \\in \\mathbb{R}^{d_{model} \\times d_k}, W_i^K \\in \\mathbb{R}^{d_{model} \\times d_k}, W^V_i \\in \\mathbb{R}^{d_{model} \\times d_v}, W^O \\in \\mathbb{R}^{hd_v \\times d_{model}}$.\n",
        "\n",
        "В оригинальной статье использовали $h=8$, $d_k=d_v=d_{\\text{model}}/h=64$.\n",
        "\n",
        "Процесс применения такой:\n",
        "![](http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png)  \n",
        "*From Illustrated Transformer*"
      ]
    },
    {
      "metadata": {
        "id": "rg-CxvPDAJPP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MultiHeadedAttention(nn.Module):\n",
        "    def __init__(self, heads_count, d_model, dropout_rate=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert d_model % heads_count == 0\n",
        "\n",
        "        self._d_k = d_model // heads_count\n",
        "        self._heads_count = heads_count\n",
        "        self._attention = ScaledDotProductAttention(dropout_rate)\n",
        "        self._attn_probs = None\n",
        "        \n",
        "        self._w_q = nn.Linear(d_model, d_model)\n",
        "        self._w_k = nn.Linear(d_model, d_model)\n",
        "        self._w_v = nn.Linear(d_model, d_model)\n",
        "        self._w_o = nn.Linear(d_model, d_model)\n",
        "        \n",
        "    def forward(self, query, key, value, mask=None):\n",
        "        <calc it>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pOKwneaKGaJi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Position-wise Feed-Forward Networks\n",
        "\n",
        "Линейный блок в энкодере выглядит так:\n",
        "$$\\mathrm{FFN}(x)=\\max(0, xW_1 + b_1) W_2 + b_2$$"
      ]
    },
    {
      "metadata": {
        "id": "uh1UVkAUGiwh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.w_1 = nn.Linear(d_model, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        return self.w_2(self.dropout(F.relu(self.w_1(inputs))))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_dZoU1JIt6QP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Encoder block\n",
        "\n",
        "Соберем все в блок:"
      ]
    },
    {
      "metadata": {
        "id": "Nh7wQL65sBmk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, size, self_attn, feed_forward, dropout_rate):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._self_attn = self_attn\n",
        "        self._feed_forward = feed_forward\n",
        "        self._self_attention_block = ResidualBlock(size, dropout_rate)\n",
        "        self._feed_forward_block = ResidualBlock(size, dropout_rate)\n",
        "\n",
        "    def forward(self, inputs, mask):\n",
        "        outputs = self._self_attention_block(inputs, lambda inputs: self._self_attn(inputs, inputs, inputs, mask))\n",
        "        return self._feed_forward_block(outputs, self._feed_forward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x8ndCRZLl4ZZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, d_ff, blocks_count, heads_count, dropout_rate):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._emb = nn.Sequential(\n",
        "            nn.Embedding(vocab_size, d_model),\n",
        "            PositionalEncoding(d_model, dropout_rate)\n",
        "        )\n",
        "        \n",
        "        block = lambda: EncoderBlock(\n",
        "            size=d_model, \n",
        "            self_attn=MultiHeadedAttention(heads_count, d_model, dropout_rate), \n",
        "            feed_forward=PositionwiseFeedForward(d_model, d_ff, dropout_rate),\n",
        "            dropout_rate=dropout_rate\n",
        "        )\n",
        "        self._blocks = nn.ModuleList([block() for _ in range(blocks_count)])\n",
        "        self._norm = LayerNorm(d_model)\n",
        "        \n",
        "    def forward(self, inputs, mask):\n",
        "        inputs = self._emb(inputs)\n",
        "        \n",
        "        for block in self._blocks:\n",
        "            inputs = block(inputs, mask)\n",
        "        return self._norm(inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_pphRcbTvqnq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Decoder\n",
        "\n",
        "![](https://hsto.org/webt/59/f0/44/59f0440f7d88f805415140.png =x550)\n",
        "\n",
        "Блок декодера (серая часть) состоит уже из трех частей:\n",
        "1. Сперва - тот же self-attention, что и в энкодере\n",
        "2. Затем - стандартный attention на выходы из энкодера + текущее состояние декодера (такой же был в seq2seq with attention)\n",
        "3. Наконец - feed-forward блок\n",
        "\n",
        "Всё это, конечно, с residual связями."
      ]
    },
    {
      "metadata": {
        "id": "6LTWjKUXx2LP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, size, self_attn, encoder_attn, feed_forward, dropout_rate):\n",
        "        super().__init__()\n",
        "                \n",
        "        self._self_attn = self_attn\n",
        "        self._encoder_attn = encoder_attn\n",
        "        self._feed_forward = feed_forward\n",
        "        self._self_attention_block = ResidualBlock(size, dropout_rate)\n",
        "        self._attention_block = ResidualBlock(size, dropout_rate)\n",
        "        self._feed_forward_block = ResidualBlock(size, dropout_rate)\n",
        " \n",
        "    def forward(self, inputs, encoder_output, source_mask, target_mask):\n",
        "        outputs = self._self_attention_block(\n",
        "            inputs, lambda inputs: self._self_attn(inputs, inputs, inputs, target_mask)\n",
        "        )\n",
        "        outputs = self._attention_block(\n",
        "            outputs, lambda inputs: self._encoder_attn(inputs, encoder_output, encoder_output, source_mask)\n",
        "        )\n",
        "        return self._feed_forward_block(outputs, self._feed_forward)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Un0AOmdqLPp_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model, d_ff, blocks_count, heads_count, dropout_rate):\n",
        "        super().__init__()\n",
        "        \n",
        "        self._emb = nn.Sequential(\n",
        "            nn.Embedding(vocab_size, d_model),\n",
        "            PositionalEncoding(d_model, dropout_rate)\n",
        "        )\n",
        "        \n",
        "        block = lambda: DecoderLayer(\n",
        "            size=d_model, \n",
        "            self_attn=MultiHeadedAttention(heads_count, d_model, dropout_rate),\n",
        "            encoder_attn=MultiHeadedAttention(heads_count, d_model, dropout_rate),\n",
        "            feed_forward=PositionwiseFeedForward(d_model, d_ff, dropout_rate),\n",
        "            dropout_rate=dropout_rate\n",
        "        )\n",
        "        self._blocks = nn.ModuleList([block() for _ in range(blocks_count)])\n",
        "        self._norm = LayerNorm(d_model)\n",
        "        self._out_layer = nn.Linear(d_model, vocab_size)\n",
        "        \n",
        "    def forward(self, inputs, encoder_output, source_mask, target_mask):\n",
        "        inputs = self._emb(inputs)\n",
        "        for block in self._blocks:\n",
        "            inputs = block(inputs, encoder_output, source_mask, target_mask)\n",
        "        return self._out_layer(self._norm(inputs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tbEkHtwWz0Yh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "В декодере нужно аттентиться только на предыдущие токены - сгенерируем маску для этого:"
      ]
    },
    {
      "metadata": {
        "id": "EbVkZSRq0cD5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def subsequent_mask(size):\n",
        "    mask = torch.ones(size, size, device=DEVICE).triu_()\n",
        "    return mask.unsqueeze(0) == 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_8X_fqeL0jeW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(5,5))\n",
        "plt.imshow(subsequent_mask(20)[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b7UEsntVj9lb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Полная модель"
      ]
    },
    {
      "metadata": {
        "id": "vLIGjPOiO7X9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class FullModel(nn.Module):\n",
        "    def __init__(self, source_vocab_size, target_vocab_size, d_model=256, d_ff=1024, \n",
        "                 blocks_count=4, heads_count=8, dropout_rate=0.1):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.d_model = d_model\n",
        "        self.encoder = Encoder(source_vocab_size, d_model, d_ff, blocks_count, heads_count, dropout_rate)\n",
        "        self.decoder = Decoder(target_vocab_size, d_model, d_ff, blocks_count, heads_count, dropout_rate)\n",
        "        \n",
        "        for p in self.parameters():\n",
        "            if p.dim() > 1:\n",
        "                nn.init.xavier_uniform_(p)\n",
        "        \n",
        "    def forward(self, source_inputs, target_inputs, source_mask, target_mask):\n",
        "        encoder_output = self.encoder(source_inputs, source_mask)\n",
        "        return self.decoder(target_inputs, encoder_output, source_mask, target_mask)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BmucSaUOjlmh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def make_mask(source_inputs, target_inputs, pad_idx):\n",
        "    source_mask = (source_inputs != pad_idx).unsqueeze(-2)\n",
        "    target_mask = (target_inputs != pad_idx).unsqueeze(-2)\n",
        "    target_mask = target_mask & subsequent_mask(target_inputs.size(-1)).type_as(target_mask)\n",
        "    return source_mask, target_mask\n",
        "\n",
        "\n",
        "def convert_batch(batch, pad_idx=1):\n",
        "    source_inputs, target_inputs = batch.source.transpose(0, 1), batch.target.transpose(0, 1)\n",
        "    source_mask, target_mask = make_mask(source_inputs, target_inputs, pad_idx)\n",
        "    \n",
        "    return source_inputs, target_inputs, source_mask, target_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9iWSl6m6jfbl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_iter))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5_qVuSL8QJg4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = FullModel(source_vocab_size=len(word_field.vocab), target_vocab_size=len(word_field.vocab)).to(DEVICE)\n",
        "\n",
        "model(*convert_batch(batch))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vX6PrVksnaq7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Оптимизатор\n",
        "\n",
        "Тоже очень важно в данной модели - использовать правильный оптимизатор"
      ]
    },
    {
      "metadata": {
        "id": "KMhopCgTnh-w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NoamOpt(object):\n",
        "    def __init__(self, model_size, factor=2, warmup=4000, optimizer=None):\n",
        "        if optimizer is not None:\n",
        "            self.optimizer = optimizer\n",
        "        else:\n",
        "            self.optimizer = optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)\n",
        "        self._step = 0\n",
        "        self.warmup = warmup\n",
        "        self.factor = factor\n",
        "        self.model_size = model_size\n",
        "        self._rate = 0\n",
        "        \n",
        "    def step(self):\n",
        "        self._step += 1\n",
        "        rate = self.rate()\n",
        "        for p in self.optimizer.param_groups:\n",
        "            p['lr'] = rate\n",
        "        self._rate = rate\n",
        "        self.optimizer.step()\n",
        "        \n",
        "    def rate(self, step = None):\n",
        "        if step is None:\n",
        "            step = self._step\n",
        "        return self.factor * (self.model_size ** (-0.5) * min(step ** (-0.5), step * self.warmup ** (-1.5)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OuYc21J5oIdb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Идея в том, чтобы повышать learning rate в течении первых warmup шагов линейно, а затем понижать его по сложной формуле:\n",
        "\n",
        "$$lrate = d_{\\text{model}}^{-0.5} \\cdot\n",
        "  \\min({step\\_num}^{-0.5},\n",
        "    {step\\_num} \\cdot {warmup\\_steps}^{-1.5})$$"
      ]
    },
    {
      "metadata": {
        "id": "kMBL261hoA58",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "opts = [NoamOpt(512, 1, 4000, None), \n",
        "        NoamOpt(512, 1, 8000, None),\n",
        "        NoamOpt(256, 1, 4000, None)]\n",
        "plt.plot(np.arange(1, 20000), [[opt.rate(i) for opt in opts] for i in range(1, 20000)])\n",
        "plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "W71i85Q4pdOS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Тренировка модели"
      ]
    },
    {
      "metadata": {
        "id": "_E2JxfRuphch",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "tqdm.get_lock().locks = []\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data_iter, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = len(data_iter)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, batch in enumerate(data_iter):\n",
        "                source_inputs, target_inputs, source_mask, target_mask = convert_batch(batch)                                \n",
        "                logits = model.forward(source_inputs, target_inputs[:, :-1], source_mask, target_mask[:, :-1, :-1])\n",
        "                \n",
        "                logits = logits.contiguous().view(-1, logits.shape[-1])\n",
        "                target = target_inputs[:, 1:].contiguous().view(-1)\n",
        "                loss = criterion(logits, target)\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(name, loss.item(), \n",
        "                                                                                         math.exp(loss.item())))\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, PPX = {:.2f}'.format(\n",
        "                name, epoch_loss / batches_count, math.exp(epoch_loss / batches_count))\n",
        "            )\n",
        "            progress_bar.refresh()\n",
        "\n",
        "    return epoch_loss / batches_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_iter, epochs_count=1, val_iter=None):\n",
        "    best_val_loss = None\n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss = do_epoch(model, criterion, train_iter, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_iter is None:\n",
        "            val_loss = do_epoch(model, criterion, val_iter, None, name_prefix + '  Val:')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5X2kYDU_rCjP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = FullModel(source_vocab_size=len(word_field.vocab), target_vocab_size=len(word_field.vocab)).to(DEVICE)\n",
        "\n",
        "pad_idx = word_field.vocab.stoi['<pad>']\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx).to(DEVICE)\n",
        "\n",
        "optimizer = NoamOpt(model.d_model)\n",
        "\n",
        "fit(model, criterion, optimizer, train_iter, epochs_count=30, val_iter=test_iter)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PRMFkzDV-wI9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Задание** Добавьте генератор для модели.\n",
        "\n",
        "**Задание** Добавьте оценку для модели с помощью ROUGE metric (например, из пакета https://pypi.org/project/pyrouge/0.1.3/)\n",
        "\n",
        "**Задание** Добавьте визуализацию (можно подсмотреть в коде по ссылкам)."
      ]
    },
    {
      "metadata": {
        "id": "Ij1GiNx54Ke5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Улучшения модели\n",
        "\n",
        "**Задание** Попробовать расшарить матрицы эмбеддингов - их тут три (входные в энкодер и декодер + выход декодера).\n",
        "\n",
        "**Задание** Замените лосс на LabelSmoothing."
      ]
    },
    {
      "metadata": {
        "id": "goFvQj18--iP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pointer-Generator Networks\n",
        "\n",
        "Клёвая идея, специфичная для саммаризации:\n",
        "\n",
        "![](https://image.ibb.co/eijTc0/2018-11-20-10-18-52.png)\n",
        "\n",
        "**Задание** Попробуйте реализовать её."
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "N4-3pYqVJIKA"
      },
      "cell_type": "markdown",
      "source": [
        "# Дополнительные материалы\n",
        "\n",
        "## Статьи\n",
        "Attention Is All You Need, 2017 [[pdf]](https://arxiv.org/pdf/1706.03762.pdf)  \n",
        "Get To The Point: Summarization with Pointer-Generator Networks, 2017 [[pdf]](https://arxiv.org/pdf/1704.04368.pdf)  \n",
        "Universal Transformers, 2018 [[arxiv]](https://arxiv.org/abs/1807.03819)\n",
        "\n",
        "## Блоги\n",
        "[Transformer — новая архитектура нейросетей для работы с последовательностями](https://habr.com/post/341240/)  \n",
        "[The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/)  \n",
        "[The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html)  \n",
        "[Weighted Tranformer](https://einstein.ai/research/blog/weighted-transformer)  \n",
        "[Your tldr by an ai: a deep reinforced model for abstractive summarization](https://einstein.ai/research/blog/your-tldr-by-an-ai-a-deep-reinforced-model-for-abstractive-summarization)"
      ]
    },
    {
      "metadata": {
        "id": "Vwb5e5hPQebd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Сдача\n",
        "\n",
        "[Форма для сдачи](https://goo.gl/forms/3npI3cPpru8JorXV2)  \n",
        "[Feedback](https://goo.gl/forms/9aizSzOUrx7EvGlG3)"
      ]
    }
  ]
}